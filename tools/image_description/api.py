import re
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from io import BytesIO
from collections import Counter

import torch
from PIL import Image, ImageOps
from ultralytics import YOLO
from huggingface_hub import snapshot_download
from transformers import (
    BlipProcessor,
    BlipForConditionalGeneration,
    CLIPProcessor,
    pipeline,
)

from fastapi import APIRouter, UploadFile, File, Form
from fastapi.responses import JSONResponse

# å¼•å…¥é¡¹ç›®å…¬å…±æ¨¡å—
from common.utils import get_logger

logger = get_logger(__name__)
router = APIRouter()

# ===== é…ç½®å‚æ•° =====
LLM_MODEL = "qwen3:0.6b"
YOLO_CONF = 0.7
YOLO_IOU = 0.7
YOLO_IMGSZ = 800

# â€”â€” æ¨¡å‹ä½ç½®ï¼ˆå»ºè®®åç»­ç§»å…¥ config.settingsï¼‰
MODEL_ROOT = Path("./yolo_models").resolve()
YOLO_MODEL_NAME = "yolov8s.pt"
YOLO_LOCAL_PATH = MODEL_ROOT / YOLO_MODEL_NAME

# è¯·ç¡®ä¿æœåŠ¡å™¨ä¸Šè¿™äº›è·¯å¾„å­˜åœ¨ï¼Œæˆ–è€…ä¿®æ”¹ä¸ºç›¸å¯¹è·¯å¾„
BLIP_LOCAL_DIR = Path("/blip/BLIP-main/blip1/")
BLIP_REPO_ID = "Salesforce/blip-image-captioning-base"

CLIP_LOCAL_DIR = Path("/blip/BLIP-main/models/clip-vit-large-patch14")
CLIP_REPO_ID = "openai/clip-vit-large-patch14"

# â€”â€” è®¾å¤‡å‚æ•°
USE_CUDA = torch.cuda.is_available()
YOLO_DEVICE = "cuda" if USE_CUDA else "cpu"
BLIP_DEVICE = "cuda" if USE_CUDA else "cpu"
CLF_DEVICE = 0 if USE_CUDA else -1
BLIP_MAX_TOKENS = 30
BLIP_USE_PROMPT = True
CLF_TOPK = 1

# â€”â€” åœºæ™¯åˆ†ç±»
CLF_LABELS_EN = [
    "a busy airport terminal with passengers and airplanes",
    "a commercial seaport with ships, cranes and containers",
    "a person or a large crowd of people in a public place",
    "a natural landscape with mountains, trees or rivers",
]
CLF_LABEL_ZH_MAP = {
    "a busy airport terminal with passengers and airplanes": "æœºåœºåœºæ™¯",
    "a commercial seaport with ships, cranes and containers": "æ¸¯å£/ç å¤´åœºæ™¯",
    "a person or a large crowd of people in a public place": "äººç‰©/äººç¾¤",
    "a natural landscape with mountains, trees or rivers": "è‡ªç„¶é£æ™¯",
}

IGNORE_CLASSES = {"remote", "vase", "tie"}

# â€”â€” LLM
try:
    from langchain_ollama import ChatOllama
    from langchain_core.prompts import PromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from langchain.schema.runnable import RunnableLambda

    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False

# å…¨å±€æ¨¡å‹å¥æŸ„
yolo_mdl = None
blip_bundle: Optional[Dict[str, Any]] = None
llm = None
_CLF_PIPE: Optional[Any] = None


# =========================
# ===== æ¨¡å‹åŠ è½½é€»è¾‘ =====
# =========================

def ensure_local_yolo_model(model_path: Path) -> Path:
    if not model_path.exists():
        logger.info(f"[YOLO] æœªæ‰¾åˆ°æœ¬åœ°æƒé‡ï¼š{model_path}ï¼Œå°è¯•ä¸‹è½½...")
        try:
            _ = YOLO(str(model_path))  # å°è¯•è§¦å‘ultralyticsä¸‹è½½
        except Exception:
            # å¦‚æœæŒ‡å®šè·¯å¾„ä¸å­˜åœ¨ä¸”æ— æ³•ä¸‹è½½ï¼Œå›é€€åˆ°é»˜è®¤åç§°ä¸‹è½½åˆ°ç¼“å­˜
            _ = YOLO(YOLO_MODEL_NAME)
    return model_path


def load_yolov8(local_path: Path) -> YOLO:
    # ä¼˜å…ˆåŠ è½½æŒ‡å®šè·¯å¾„ï¼Œå¦åˆ™åŠ è½½é»˜è®¤
    target = str(local_path) if local_path.exists() else YOLO_MODEL_NAME
    mdl = YOLO(target)
    try:
        mdl.to(YOLO_DEVICE)
    except Exception:
        pass
    return mdl


def _blip_files_ok(dir_: Path) -> bool:
    if not dir_.exists(): return False
    must_have = ["config.json", "tokenizer.json"]
    return all((dir_ / f).exists() for f in must_have)


def ensure_local_blip(local_dir: Path, repo_id: str) -> Path:
    if _blip_files_ok(local_dir):
        return local_dir
    logger.info(f"[BLIP] æœ¬åœ°ç¼ºå¤±ï¼Œä¸‹è½½ {repo_id} -> {local_dir}")
    local_dir.mkdir(parents=True, exist_ok=True)
    snapshot_download(repo_id=repo_id, local_dir=str(local_dir))
    return local_dir


def load_blip(local_dir: Path) -> Optional[Dict[str, Any]]:
    if not _blip_files_ok(local_dir):
        return None
    try:
        processor = BlipProcessor.from_pretrained(str(local_dir), local_files_only=True)
        model = BlipForConditionalGeneration.from_pretrained(str(local_dir), local_files_only=True)
        model.to(BLIP_DEVICE)
        model.eval()
        return {"processor": processor, "model": model}
    except Exception as e:
        logger.error(f"[BLIP] Load error: {e}")
        return None


def _clip_files_ok(dir_: Path) -> bool:
    return dir_.exists() and (dir_ / "config.json").exists()


def ensure_local_clip(local_dir: Path, repo_id: str) -> Path:
    if _clip_files_ok(local_dir):
        return local_dir
    logger.info(f"[CLIP] æœ¬åœ°ç¼ºå¤±ï¼Œä¸‹è½½ {repo_id} -> {local_dir}")
    local_dir.mkdir(parents=True, exist_ok=True)
    snapshot_download(repo_id=repo_id, local_dir=str(local_dir))
    return local_dir


def load_clip_classifier():
    global _CLF_PIPE
    if _CLF_PIPE is not None:
        return _CLF_PIPE

    local_dir = ensure_local_clip(CLIP_LOCAL_DIR, CLIP_REPO_ID)
    try:
        # å°è¯•åŠ è½½ CLIP pipeline
        _CLF_PIPE = pipeline(
            "zero-shot-image-classification",
            model=str(local_dir),
            device=CLF_DEVICE,
        )
    except Exception as e:
        logger.error(f"[CLIP] Load error: {e}")
        _CLF_PIPE = None
    return _CLF_PIPE


# =========================
# ===== ä¸šåŠ¡é€»è¾‘å‡½æ•° =====
# =========================

def fmt_score(x: float) -> str:
    return f"{x:.4f}"


def _collect_ultra(results) -> List[Dict[str, Any]]:
    dets = []
    for r in results:
        if getattr(r, "boxes", None) is None: continue
        for box in r.boxes:
            score = float(box.conf[0].item())
            cls_id = int(box.cls[0].item())
            name = r.names.get(cls_id, str(cls_id))
            dets.append({"name": name, "score": score})
    return sorted(dets, key=lambda d: d["score"], reverse=True)


def detect_elements_yolo(img: Image.Image) -> List[Dict[str, Any]]:
    if not yolo_mdl: return []
    results = yolo_mdl.predict(source=img, conf=YOLO_CONF, iou=YOLO_IOU, device=YOLO_DEVICE, imgsz=YOLO_IMGSZ,
                               verbose=False, max_det=300)
    raw_dets = _collect_ultra(results)

    # å»é‡ä¿ç•™æœ€é«˜åˆ†
    best = {}
    for d in raw_dets:
        nm = d["name"]
        if nm not in best or d["score"] > best[nm]["score"]:
            best[nm] = d
    return sorted(best.values(), key=lambda x: x["score"], reverse=True)


@torch.no_grad()
def generate_caption_blip(image: Image.Image) -> str:
    if not blip_bundle: return ""
    proc = blip_bundle["processor"]
    model = blip_bundle["model"]

    inputs = proc(image, text="a photo of" if BLIP_USE_PROMPT else None, return_tensors="pt").to(BLIP_DEVICE)
    out = model.generate(**inputs, max_new_tokens=BLIP_MAX_TOKENS, min_new_tokens=5)
    return proc.decode(out[0], skip_special_tokens=True).strip()


def classify_scene_clip(img: Image.Image) -> List[Dict[str, Any]]:
    clf = load_clip_classifier()
    if not clf: return []

    try:
        res = clf(img, candidate_labels=CLF_LABELS_EN)
        # res æ˜¯ list[dict]
        # æ‰¾åˆ°æœ€é«˜åˆ†
        best_score = 0.0
        best_label = ""
        for item in res:
            if item["score"] > best_score:
                best_score = item["score"]
                best_label = item["label"]

        if best_score < 0.4: return []

        return [{
            "label_en": best_label,
            "label_zh": CLF_LABEL_ZH_MAP.get(best_label, best_label),
            "score": best_score
        }]
    except Exception as e:
        logger.error(f"[CLIP] Inference error: {e}")
        return []


# ... (è¾…åŠ©æ–‡æœ¬å¤„ç†å‡½æ•°: strip_think, translate_to_zh, extract_news_entities ç­‰) ...
# ä¸ºèŠ‚çœç¯‡å¹…ï¼Œç›´æ¥å¤ç”¨ä¹‹å‰è„šæœ¬é‡Œçš„é€»è¾‘ï¼Œè¿™é‡Œç®€åŒ–å®ç°

def strip_think(text: str) -> str:
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()


def translate_to_zh(llm_obj, text: str) -> str:
    if not text or not llm_obj: return text
    try:
        prompt = PromptTemplate.from_template("å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¸å¸¦è§£é‡Šï¼š\n{text}")
        chain = prompt | llm_obj | StrOutputParser() | RunnableLambda(strip_think)
        return chain.invoke({"text": text}).strip()
    except:
        return text


def fuse_zh_multi(llm_obj, scenes, targets, blips, news_text):
    if not llm_obj:
        return f"åœºæ™¯:{scenes}; ç›®æ ‡:{targets}; æè¿°:{blips}"

    prompt = PromptTemplate.from_template(
        """åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€æ®µä¸­æ–‡å›¾ç‰‡æè¿°ï¼š
        æ–°é—»æ–‡æœ¬: {news}
        åœºæ™¯åˆ†ç±»: {scenes}
        æ£€æµ‹ç›®æ ‡: {targets}
        åŸºç¡€æè¿°: {blips}

        è¦æ±‚ï¼šè‡ªç„¶è¿è´¯ï¼Œä»¥è§†è§‰ä¿¡æ¯ä¸ºä¸»ã€‚
        è¾“å‡ºæ ¼å¼ï¼š
        label: [æè¿°å†…å®¹]
        """
    )
    chain = prompt | llm_obj | StrOutputParser() | RunnableLambda(strip_think)
    try:
        res = chain.invoke({
            "news": news_text[:200], "scenes": str(scenes),
            "targets": str(targets), "blips": str(blips)
        })
        if "label:" in res:
            return res.split("label:", 1)[1].strip()
        return res
    except:
        return "ç”Ÿæˆå¤±è´¥"


# =========================
# ===== æ ¸å¿ƒåˆ†æé€»è¾‘ =====
# =========================

def analyze_images_group(text: str, images_bytes: List[bytes]) -> dict:
    scenes_all = []
    targets_all = []
    blips_all = []

    for b in images_bytes:
        try:
            img = Image.open(BytesIO(b)).convert("RGB")

            # 1. åœºæ™¯
            s_list = classify_scene_clip(img)
            s_str = s_list[0]["label_zh"] if s_list else "æœªçŸ¥"
            scenes_all.append(s_str)

            # 2. ç›®æ ‡
            dets = detect_elements_yolo(img)
            t_str = ",".join([d['name'] for d in dets[:5]])
            targets_all.append(t_str)

            # 3. æè¿°
            blip_en = generate_caption_blip(img)
            blip_zh = translate_to_zh(llm, blip_en)
            blips_all.append(blip_zh)

        except Exception as e:
            logger.error(f"Image process error: {e}")

    # èåˆç”Ÿæˆ
    desc = fuse_zh_multi(llm, scenes_all, targets_all, blips_all, text)

    # ç®€å•èšåˆç±»åˆ«
    category = Counter(scenes_all).most_common(1)[0][0] if scenes_all else "æœªåˆ†ç±»"

    # èšåˆå…ƒç´ 
    all_elements = set()
    for t in targets_all:
        for x in t.split(','):
            if x.strip(): all_elements.add(x.strip())

    # ç¿»è¯‘å…ƒç´ 
    elements_zh = [translate_to_zh(llm, e) for e in list(all_elements)[:10]]

    return {
        "category": category,
        "elements": elements_zh,
        "description_zh": desc
    }


# =========================
# ===== è·¯ç”±ä¸é¢„çƒ­ =====
# =========================

@router.post("/analyze", summary="å¤šæ¨¡æ€å›¾ç‰‡åˆ†æ")
async def images_des_analyze(
        text: str = Form(""),
        images: List[UploadFile] = File(...)
):
    if not images:
        return JSONResponse(status_code=400, content={"detail": "è¯·ä¸Šä¼ å›¾ç‰‡"})

    imgs_bytes = []
    for f in images:
        imgs_bytes.append(await f.read())

    if not yolo_mdl and not blip_bundle:
        return JSONResponse(status_code=500, content={"detail": "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·ç­‰å¾…æœåŠ¡å¯åŠ¨å®Œæˆ"})

    # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œè€—æ—¶æ“ä½œ
    # Fastapi çš„ async def ä¸­è¿è¡Œ CPU å¯†é›†å‹ä»»åŠ¡ä¼šé˜»å¡
    # ä½†ç”±äºè¿™é‡Œä¸»è¦æ˜¯æ¨¡å‹æ¨ç†ï¼ŒçŸ­æœŸå†…ç›´æ¥è°ƒç”¨ä¹Ÿå¯ä»¥ï¼Œæˆ–è€…ä½¿ç”¨ run_in_threadpool
    try:
        result = analyze_images_group(text, imgs_bytes)
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"Analyze error: {e}")
        return JSONResponse(status_code=500, content={"detail": str(e)})


async def warmup():
    """ç³»ç»Ÿå¯åŠ¨æ—¶çš„é¢„çƒ­å‡½æ•°"""
    global yolo_mdl, blip_bundle, llm

    logger.info("ğŸ”¥ [ImageDesc] å¼€å§‹åŠ è½½å¤šæ¨¡æ€æ¨¡å‹...")

    # 1. YOLO
    try:
        ensure_local_yolo_model(YOLO_LOCAL_PATH)
        yolo_mdl = load_yolov8(YOLO_LOCAL_PATH)
    except Exception as e:
        logger.error(f"âŒ YOLO åŠ è½½å¤±è´¥: {e}")

    # 2. BLIP
    try:
        blip_dir = ensure_local_blip(BLIP_LOCAL_DIR, BLIP_REPO_ID)
        blip_bundle = load_blip(blip_dir)
    except Exception as e:
        logger.error(f"âŒ BLIP åŠ è½½å¤±è´¥: {e}")

    # 3. CLIP
    try:
        load_clip_classifier()
    except Exception as e:
        logger.error(f"âŒ CLIP åŠ è½½å¤±è´¥: {e}")

    # 4. LLM
    if LLM_AVAILABLE:
        try:
            llm = ChatOllama(model=LLM_MODEL, temperature=0.3)
            # ç®€å•æµ‹è¯•
            # await llm.ainvoke("ping")
            logger.info(f"âœ… LLM ({LLM_MODEL}) è¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ LLM è¿æ¥å¤±è´¥: {e}")
            llm = None
    else:
        logger.warning("âš ï¸ æœªå®‰è£… langchain_ollamaï¼ŒLLM åŠŸèƒ½ç¦ç”¨")

    logger.info("âœ… [ImageDesc] æ¨¡å‹åŠ è½½æµç¨‹ç»“æŸ")