# semantic/Into_mongodb/mogongdb.py
import os
import csv
import json
import re
from datetime import datetime
from collections import defaultdict

from pymongo import MongoClient
from bson import ObjectId

# ã€ä¿®æ”¹ç‚¹ 1ã€‘å¼•å…¥ settingsï¼Œç§»é™¤ config.relation_whitelist
from config.settings import settings

# --- æ–‡ä»¶è¾“å‡ºç›®å½•ï¼ˆCSV / JSONï¼‰---
# ä½¿ç”¨ settings.BASE_DIR ç›¸å¯¹è·¯å¾„ï¼Œé¿å…ç¡¬ç¼–ç 
CSV_DIR = os.path.join(settings.BASE_DIR, "csv_output")
os.makedirs(CSV_DIR, exist_ok=True)

EVENT_EVENT_EDGE_FILE = os.path.join(CSV_DIR, "event_event_edges.csv")  # äº‹ä»¶-äº‹ä»¶è¾¹
EVENT_GOAL_EDGE_FILE = os.path.join(CSV_DIR, "event_goal.csv")  # äº‹ä»¶-ç›®æ ‡è¾¹ï¼ˆä¸­é—´æ–‡ä»¶ï¼‰

SUCCESS_CSV = os.path.join(CSV_DIR, "success.csv")  # æ˜ å°„æˆåŠŸçš„äº‹ä»¶-ç›®æ ‡è¾¹ï¼ˆdst å·²æ˜¯ target_idï¼‰
FAILED_CSV = os.path.join(CSV_DIR, "failed.csv")  # æ˜ å°„å¤±è´¥çš„äº‹ä»¶-ç›®æ ‡è¾¹
SUCCESS_JSON = os.path.join(CSV_DIR, "success.json")  # æ˜ å°„æˆåŠŸæ¶‰åŠçš„ç›®æ ‡èŠ‚ç‚¹ä¿¡æ¯ï¼ˆå»é‡ï¼‰

TARGET_TARGET_EDGE_FILE = os.path.join(CSV_DIR, "target_target_edges.csv")  # ç›®æ ‡-ç›®æ ‡è¾¹
TARGET_TARGET_FAILED_FILE = os.path.join(CSV_DIR, "target_target_edges_failed.csv")  # ç›®æ ‡-ç›®æ ‡è¾¹åŒ¹é…å¤±è´¥è®°å½•

# --- æ ¹æ®äº‹ä»¶-ç›®æ ‡å…³ç³»æ¨æ–­ç›®æ ‡ç±»å‹ â†’ å¯¹åº” knowledge_target çš„ event_second_level ---
TYPE_TO_EVENT_LEVEL = {
    "character": "äººç‰©ç›®æ ‡",
    "organization": "ç»„ç»‡ç›®æ ‡",
    "facility": ["æœºåœºç›®æ ‡", "æ¸¯å£ç›®æ ‡"],
    "unknow": ["äººç‰©ç›®æ ‡", "ç»„ç»‡ç›®æ ‡"],  # å…œåº•
}


# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================

def is_event_id(x: str) -> bool:
    """åˆ¤æ–­ä¸€ä¸ª ID æ˜¯å¦æ˜¯äº‹ä»¶ IDï¼šTh_ / Th- / TH_ / TH- å¼€å¤´"""
    x = (x or "").strip()
    upper = x.upper()
    return upper.startswith("TH_") or upper.startswith("TH-")


def infer_target_type_from_relation(rel: str) -> str:
    """æ ¹æ® relation æ¨æ–­ç›®æ ‡ç±»å‹ï¼šorganization / character / facility / unknow"""
    rel = (rel or "").lower()
    if "organization" in rel:
        return "organization"
    if "person" in rel:
        return "character"
    if "facility" in rel:
        return "facility"
    return "unknow"


def json_converter(obj):
    if isinstance(obj, datetime):
        return obj.isoformat()
    if isinstance(obj, ObjectId):
        return str(obj)
    return str(obj)


def load_csv(file_path):
    with open(file_path, mode='r', encoding='utf-8-sig') as f:
        return list(csv.DictReader(f))


def collect_fieldnames(rows):
    fields = []
    for r in rows:
        for k in r.keys():
            if k not in fields:
                fields.append(k)
    return fields


def save_csv(file_path, rows):
    if not rows:
        return
    fieldnames = collect_fieldnames(rows)
    with open(file_path, mode='w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)


def save_json(file_path, data):
    if not data:
        return
    with open(file_path, mode='w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=json_converter)


def split_alias(alias_str):
    """æ‹†åˆ†åˆ«åï¼šå…¼å®¹ é¡¿å·/é€—å·/åˆ†å·/ç«–çº¿ ç­‰"""
    if not alias_str:
        return []
    seps = ["ã€", ",", "ï¼Œ", ";", "ï¼›", "|", "â€†", "â€ƒ"]
    tmp = [alias_str]
    for s in seps:
        alias_list = []
        for part in tmp:
            alias_list.extend(part.split(s))
        tmp = alias_list
    return [x.strip() for x in tmp if x and x.strip()]


def flatten_dict(d, parent_key='', sep='.'):
    """å±•å¹³åµŒå¥—å­—å…¸ / åˆ—è¡¨"""
    items = []
    if not isinstance(d, dict):
        return {}
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        elif isinstance(v, list):
            for idx, sub_v in enumerate(v):
                if isinstance(sub_v, dict):
                    items.extend(flatten_dict(sub_v, f"{new_key}{sep}{idx}", sep=sep).items())
                else:
                    items.append((f"{new_key}{sep}{idx}", sub_v))
        else:
            items.append((new_key, v))
    return dict(items)


def build_name_alias_index(event_second_level, collection_knowledge):
    """
    åœ¨ knowledge_target ä¸Šæ„å»º name/alias â†’ (target_id, data) ç´¢å¼•
    """
    index = {}

    if isinstance(event_second_level, list):
        query = {"event_second_level": {"$in": event_second_level}}
    else:
        query = {"event_second_level": event_second_level}

    for doc in collection_knowledge.find(query):
        data = doc.get("data", {})
        target_id = doc.get("target_id", "")

        if isinstance(data, dict):
            name = (data.get("name") or "").strip()
            alias = (data.get("alias") or "").strip()
            if name and name not in index:
                index[name] = (target_id, data)
            if alias:
                for a in split_alias(alias):
                    if a and a not in index:
                        index[a] = (target_id, data)

        elif isinstance(data, list):
            for record in data:
                if isinstance(record, dict):
                    name = (record.get("name") or "").strip()
                    alias = (record.get("alias") or "").strip()
                    tid = record.get("target_id", target_id)
                    if name and name not in index:
                        index[name] = (tid, record)
                    if alias:
                        for a in split_alias(alias):
                            if a and a not in index:
                                index[a] = (tid, record)

    return index


def match_value_to_ids(value, index):
    """æŠŠä¸€ä¸ªå­—æ®µå€¼æ˜ å°„åˆ°è‹¥å¹² target_id"""
    hits = []
    if not value:
        return hits

    parts = split_alias(value)
    if not parts:
        parts = [str(value).strip()]

    for token in parts:
        token = token.strip()
        if not token:
            continue
        match = index.get(token)
        if match:
            tid, _ = match
            if tid not in hits:
                hits.append(tid)
    return hits


def normalize_leader_name(raw):
    """å»é™¤èŒåŠ¡è¯´æ˜ï¼Œä¾‹å¦‚ï¼š'å­™åŠ›ï¼ˆå›½é™…è°ƒè§£é™¢ç­¹å¤‡åŠå…¬å®¤ä¸»ä»»ï¼‰' -> 'å­™åŠ›'"""
    if not raw:
        return ""
    name = re.split(r"[ï¼ˆ(]", str(raw), 1)[0]
    return name.strip()


# ============================================================
# æ­¥éª¤ 1ï¼šä» interim æå– & æ¸…æ´—äº‹ä»¶è¾¹
# ============================================================

def extract_and_clean_edges(collection_interim):
    """
    ä» MongoDB è¯»å– structured_data.event_relationsï¼Œæ‹†åˆ†å¹¶æ¸…æ´—è¾¹
    """
    print("ğŸ” æ­£åœ¨ä» MongoDB æå– structured_data.event_relations ...")

    event_event_edges = []
    event_goal_edges = []
    removed_count = 0

    seen_event_event = set()
    seen_event_goal = set()

    cursor = collection_interim.find(
        {},
        {"_id": 0, "structured_data.event_relations": 1}
    )

    for doc in cursor:
        relations = doc.get("structured_data", {}).get("event_relations", [])

        for relation_str in relations:
            if not relation_str:
                continue

            parts = relation_str.strip().split()
            if len(parts) != 3:
                removed_count += 1
                continue

            src, dst, rel = parts
            src = src.strip()
            dst = dst.strip()
            rel = rel.strip()

            if not src or not dst or not rel:
                removed_count += 1
                continue

            if src == dst:
                removed_count += 1
                continue

            src_is_event = is_event_id(src)
            dst_is_event = is_event_id(dst)

            # ---- äº‹ä»¶-äº‹ä»¶è¾¹ ----
            # ã€ä¿®æ”¹ç‚¹ 2ã€‘ä½¿ç”¨ settings.ALLOWED_EVENT_EVENT_RELATIONS
            if src_is_event and dst_is_event and rel in settings.ALLOWED_EVENT_EVENT_RELATIONS:
                key = (src, rel, dst)
                if key not in seen_event_event:
                    seen_event_event.add(key)
                    event_event_edges.append({
                        "src_event_id": src,
                        "relation": rel,
                        "dst_event_id": dst,
                    })
                continue

            # ---- äº‹ä»¶-ç›®æ ‡è¾¹ ----
            # ã€ä¿®æ”¹ç‚¹ 3ã€‘ä½¿ç”¨ settings.ALLOWED_EVENT_TARGET_RELATIONS
            if src_is_event and (not dst_is_event) and rel in settings.ALLOWED_EVENT_TARGET_RELATIONS:
                key = (src, rel, dst)
                if key not in seen_event_goal:
                    seen_event_goal.add(key)
                    target_type = infer_target_type_from_relation(rel)
                    event_goal_edges.append({
                        "src_id": src,
                        "relation": rel,
                        "dst_id": dst,
                        "type": target_type,
                    })
                continue

            # å…¶ä»–æƒ…å†µä¸¢å¼ƒ
            removed_count += 1

    with open(EVENT_EVENT_EDGE_FILE, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=["src_event_id", "relation", "dst_event_id"])
        writer.writeheader()
        writer.writerows(event_event_edges)

    with open(EVENT_GOAL_EDGE_FILE, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=["src_id", "relation", "dst_id", "type"])
        writer.writeheader()
        writer.writerows(event_goal_edges)

    print("âœ… æå– & åˆæ­¥æ¸…æ´—å®Œæˆï¼š")
    print(f"   Â· äº‹ä»¶-äº‹ä»¶è¾¹ï¼š{len(event_event_edges)} æ¡")
    print(f"   Â· äº‹ä»¶-ç›®æ ‡è¾¹ï¼š{len(event_goal_edges)} æ¡")
    print(f"   Â· ä¸¢å¼ƒæ— æ•ˆè®°å½•ï¼š{removed_count} æ¡")


# ============================================================
# æ­¥éª¤ 2ï¼šå¯¹äº‹ä»¶-ç›®æ ‡è¾¹å°±åœ°å»é‡
# ============================================================

def dedup_event_goal_edges_inplace():
    """å¯¹ event_goal.csv æŒ‰ (src_id, dst_id) å»é‡"""
    if not os.path.exists(EVENT_GOAL_EDGE_FILE):
        print(f"âš ï¸ æœªæ‰¾åˆ°äº‹ä»¶ç›®æ ‡è¾¹æ–‡ä»¶ï¼š{EVENT_GOAL_EDGE_FILE}ï¼Œè·³è¿‡å»é‡ã€‚")
        return

    print("ğŸ§¹ æ­£åœ¨å¯¹äº‹ä»¶-ç›®æ ‡è¾¹æŒ‰ (src_id, dst_id) å»é‡...")

    rows = load_csv(EVENT_GOAL_EDGE_FILE)
    seen_pairs = set()
    cleaned = []
    duplicates = []

    for r in rows:
        src = (r.get("src_id") or "").strip()
        dst = (r.get("dst_id") or "").strip()
        key = (src, dst)
        if key in seen_pairs:
            duplicates.append(r)
        else:
            seen_pairs.add(key)
            cleaned.append(r)

    if duplicates:
        print("ğŸ” æ£€æµ‹åˆ°é‡å¤çš„äº‹ä»¶-ç›®æ ‡è¾¹ï¼Œå°†è¢«å»é‡ã€‚")
        save_csv(EVENT_GOAL_EDGE_FILE, cleaned)
        print(f"âœ… å»é‡å®Œæˆï¼Œä¿å­˜åˆ°ï¼š{EVENT_GOAL_EDGE_FILE}")
    else:
        print("âœ… æ— éœ€å»é‡ã€‚")


# ============================================================
# æ­¥éª¤ 3ï¼šäº‹ä»¶-ç›®æ ‡è¾¹æ˜ å°„åˆ° knowledge_target
# ============================================================

def map_event_goal_edges(collection_knowledge):
    """
    æ˜ å°„ dst_id åˆ° target_idï¼Œç”Ÿæˆ success.csv å’Œ failed.csv
    """
    if not os.path.exists(EVENT_GOAL_EDGE_FILE):
        print(f"âš ï¸ æœªæ‰¾åˆ°äº‹ä»¶ç›®æ ‡è¾¹æ–‡ä»¶ï¼Œæ— æ³•æ˜ å°„ã€‚")
        return

    rows = load_csv(EVENT_GOAL_EDGE_FILE)

    # é¢„æ„å»ºç´¢å¼•
    indices = {}
    for t, ev in TYPE_TO_EVENT_LEVEL.items():
        print(f"æ„å»ºç´¢å¼•ï¼štype={t}, event_second_level={ev}")
        indices[t] = build_name_alias_index(ev, collection_knowledge)

    success_rows_all = []
    failed_rows_all = []
    success_json_all = []

    json_seen_target_ids = set()
    existing_pairs_by_type = defaultdict(set)

    for line_no, row in enumerate(rows, start=1):
        dst_name = (row.get("dst_id") or "").strip()
        type_raw = (row.get("type") or "").strip()
        src_id_val = (row.get("src_id") or "").strip()
        relation_val = row.get("relation")

        if type_raw not in TYPE_TO_EVENT_LEVEL:
            failed_rows_all.append(row)
            continue

        index = indices.get(type_raw) or {}
        alias_parts = split_alias(dst_name)
        if not alias_parts:
            alias_parts = [dst_name]

        match_found = False

        for part in alias_parts:
            if not part: continue
            match = index.get(part)
            if not match: continue

            target_id, data = match
            pair = (src_id_val, target_id)

            if pair in existing_pairs_by_type[type_raw]:
                continue

            new_row = dict(row)
            new_row["dst_id"] = target_id
            if "type" in new_row:
                del new_row["type"]
            success_rows_all.append(new_row)

            if target_id not in json_seen_target_ids:
                success_json_all.append({
                    "target_id": target_id,
                    "name": dst_name,
                    "data": data,
                })
                json_seen_target_ids.add(target_id)

            existing_pairs_by_type[type_raw].add(pair)
            match_found = True
            break

        if not match_found:
            failed_rows_all.append(row)

    if success_rows_all:
        save_csv(SUCCESS_CSV, success_rows_all)
        print(f"âœ… æˆåŠŸè®°å½•ä¿å­˜åˆ°ï¼š{SUCCESS_CSV} (å…± {len(success_rows_all)} æ¡)")

    if failed_rows_all:
        save_csv(FAILED_CSV, failed_rows_all)
        print(f"âš ï¸ å¤±è´¥è®°å½•ä¿å­˜åˆ°ï¼š{FAILED_CSV} (å…± {len(failed_rows_all)} æ¡)")

    if success_json_all:
        save_json(SUCCESS_JSON, success_json_all)
        print(f"ğŸ“„ æˆåŠŸ JSON ä¿å­˜åˆ°ï¼š{SUCCESS_JSON}")


# ============================================================
# æ­¥éª¤ 4ï¼šç›®æ ‡-ç›®æ ‡è¾¹æŠ½å–
# ============================================================

def extract_target_target_edges(collection_knowledge):
    """
    æŠ½å–ç›®æ ‡ä¹‹é—´çš„å…³ç³»è¾¹
    """
    if os.path.exists(SUCCESS_JSON):
        with open(SUCCESS_JSON, "r", encoding="utf-8") as f:
            success_records = json.load(f)
        allowed_src_ids = {str(rec.get("target_id")) for rec in success_records if rec.get("target_id")}
        print(f"ğŸ“¦ ä» success.json è¯»å–åˆ° {len(allowed_src_ids)} ä¸ªç›®æ ‡ä½œä¸º srcã€‚")
    else:
        allowed_src_ids = None
        print(f"âš ï¸ æœªæ‰¾åˆ° success.jsonï¼Œå…¨é‡æŠ½å–ã€‚")

    print("ğŸ” æ„å»ºå„ç±»ç›®æ ‡ç´¢å¼•...")
    person_index = build_name_alias_index("äººç‰©ç›®æ ‡", collection_knowledge)
    org_index = build_name_alias_index("ç»„ç»‡ç›®æ ‡", collection_knowledge)

    edges_seen = set()

    with open(TARGET_TARGET_EDGE_FILE, "w", newline="", encoding="utf-8-sig") as edges_f, \
            open(TARGET_TARGET_FAILED_FILE, "w", newline="", encoding="utf-8-sig") as failed_f:

        edges_writer = csv.DictWriter(edges_f, fieldnames=["src_id", "relation", "dst_id"])
        failed_writer = csv.DictWriter(failed_f, fieldnames=["src_id", "relation", "dst_raw"])
        edges_writer.writeheader()
        failed_writer.writeheader()

        total_edges = 0

        query = {"target_id": {"$in": list(allowed_src_ids)}} if allowed_src_ids else {}

        print("ğŸ”„ æ­£åœ¨æŠ½å–ç›®æ ‡-ç›®æ ‡è¾¹...")

        for doc in collection_knowledge.find(query):
            src_id = doc.get("target_id")
            ev2 = doc.get("event_second_level")
            data = doc.get("data", {}) or {}

            if not src_id or not ev2:
                continue

            # è¾…åŠ©å‡½æ•°ï¼šå¤„ç†å•ä¸ªå…³ç³»å­—æ®µ
            def process_relation(field_name, relation_name, target_index):
                nonlocal total_edges
                raw_val = data.get(field_name)
                if raw_val:
                    # ç‰¹æ®Šå¤„ç†ï¼šleader å­—æ®µå»èŒåŠ¡
                    search_val = normalize_leader_name(raw_val) if field_name == "leader" else raw_val
                    dst_ids = match_value_to_ids(search_val, target_index)
                    if dst_ids:
                        for dst in dst_ids:
                            key = (src_id, relation_name, dst)
                            if key not in edges_seen:
                                edges_seen.add(key)
                                edges_writer.writerow({"src_id": src_id, "relation": relation_name, "dst_id": dst})
                                total_edges += 1
                    else:
                        failed_writer.writerow({"src_id": src_id, "relation": relation_name, "dst_raw": raw_val})

            if ev2 == "äººç‰©ç›®æ ‡":
                process_relation("affiliated_organization", "person-personOrganization-affiliated", org_index)
            elif ev2 == "ç»„ç»‡ç›®æ ‡":
                process_relation("superior_organization", "organization-organization-superior", org_index)
                process_relation("subordinate_units", "organization-organization-subordinate", org_index)
                process_relation("international_cooperation", "organization-organization-joint", org_index)
                process_relation("leader", "target-target-position-leader", person_index)
            elif ev2 == "æœºåœºç›®æ ‡":
                process_relation("operation_operator", "target-target-operation-operatedBy", org_index)
            elif ev2 == "æ¸¯å£ç›®æ ‡":
                process_relation("operator", "target-target-operation-operatedBy", org_index)
            elif ev2 == "å†›èˆ°ç›®æ ‡":
                process_relation("command_unit", "target-target-operation-commandedBy", org_index)

        print(f"âœ… ç›®æ ‡-ç›®æ ‡è¾¹æŠ½å–å®Œæˆï¼šå†™å…¥ {total_edges} æ¡è¾¹")


# ============================================================
# æ­¥éª¤ 5ï¼šCSV å…¥åº“åˆ° EDGES_COLLECTION
# ============================================================

def import_edges_to_mongo(db):
    """
    å°†æ‰€æœ‰ç”Ÿæˆçš„è¾¹å¯¼å…¥ MongoDB
    """
    collection = db[settings.EDGES_COLLECTION]
    collection.delete_many({})
    print(f"ğŸ§¹ å·²æ¸…ç©º {settings.EDGES_COLLECTION} é›†åˆ")

    def load_and_insert(file_path, name):
        count = 0
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8-sig") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # æ ¹æ®ä¸åŒæ–‡ä»¶æ˜ å°„å­—æ®µå
                    src = row.get("src_id") or row.get("src_event_id")
                    dst = row.get("dst_id") or row.get("dst_event_id")
                    rel = row.get("relation")
                    if src and dst and rel:
                        collection.insert_one({
                            "src": src.strip(),
                            "dst": dst.strip(),
                            "relation": rel.strip()
                        })
                        count += 1
        print(f"âœ… å¯¼å…¥ {count} æ¡ {name}")

    load_and_insert(SUCCESS_CSV, "äº‹ä»¶-ç›®æ ‡ è¾¹")
    load_and_insert(EVENT_EVENT_EDGE_FILE, "äº‹ä»¶-äº‹ä»¶ è¾¹")
    load_and_insert(TARGET_TARGET_EDGE_FILE, "ç›®æ ‡-ç›®æ ‡ è¾¹")


# ============================================================
# æ­¥éª¤ 6ï¼šä»è¾¹é›†åˆæå–äº‹ä»¶èŠ‚ç‚¹å¹¶å±•å¹³å…¥åº“
# ============================================================

def extract_event_nodes_from_edges(db):
    """
    æå–æ¶‰åŠçš„äº‹ä»¶èŠ‚ç‚¹å¹¶å…¥åº“
    """
    source_col = db[settings.INTERIM_COLLECTION]
    edges_col = db[settings.EDGES_COLLECTION]
    final_nodes_col = db[settings.EVENT_NODE_COLLECTION]

    event_ids_from_edges = set()
    for edge in edges_col.find({}, {"src": 1, "dst": 1}):
        src = edge.get("src")
        dst = edge.get("dst")
        if src and src.startswith("Th"): event_ids_from_edges.add(src)
        if dst and dst.startswith("Th"): event_ids_from_edges.add(dst)

    print(f"ğŸ” æ‰¾åˆ° {len(event_ids_from_edges)} ä¸ªå”¯ä¸€çš„äº‹ä»¶ID")

    # ä¸å†æ¸…ç©ºé›†åˆï¼Œåªç»Ÿè®¡æ–°å¢ / æ›´æ–°
    insert_count = 0
    update_count = 0

    for event_id_to_find in event_ids_from_edges:
        source_doc = source_col.find_one({
            "structured_data.events.event_id": event_id_to_find
        })

        if not source_doc:
            continue

        doc_id = str(source_doc.get("_id"))
        source = source_doc.get("source", "")
        first_level = source_doc.get("event_first_level", "")
        second_level = source_doc.get("event_second_level", "")

        structured_events = source_doc.get("structured_data", {}).get("events", [])
        event_data = next((e for e in structured_events if e.get("event_id") == event_id_to_find), None)

        if not event_data:
            continue

        flat = {
            "source_doc_id": doc_id,
            "source": source,
            "event_first_level": first_level,
            "event_second_level": second_level,
            "event_id": event_id_to_find,
            "event_name": event_data.get("event_name", ""),
            "status": "0"
        }

        # å±•å¹³å„ä¸ªå­å­—æ®µ
        time_pos = event_data.get("time_position", {})
        if isinstance(time_pos, dict):
            for k, v in time_pos.items(): flat[f"time_position_{k}"] = v

        space_pos = event_data.get("space_position", {})
        if isinstance(space_pos, dict):
            for k, v in space_pos.items(): flat[f"space_position_{k}"] = v

        rel = event_data.get("relationship_characteristics", {})
        if isinstance(rel, dict):
            for role, role_info in rel.items():
                if isinstance(role_info, dict):
                    name = role_info.get("name")
                    if name: flat[f"relationship_characteristics_{role}"] = name

        attr = event_data.get("attribute_characteristics", {})
        if isinstance(attr, dict):
            for k, v in attr.items(): flat[f"attribute_{k}"] = v

        emo = event_data.get("emotion_characteristics", {})
        if isinstance(emo, dict) and emo:
            flat["emotion_characteristics"] = "ï¼›".join(emo.keys())

        # ä»¥ event_id ä¸ºå”¯ä¸€é”®åš upsert
        result = final_nodes_col.update_one(
            {"event_id": event_id_to_find},  # æŸ¥è¯¢æ¡ä»¶
            {"$set": flat},  # æ›´æ–°å†…å®¹
            upsert=True  # ä¸å­˜åœ¨å°±æ’å…¥
        )

        # ç»Ÿè®¡æ˜¯æ–°å¢è¿˜æ˜¯æ›´æ–°
        if result.matched_count == 0:
            insert_count += 1  # æ–°æ’å…¥
        else:
            update_count += 1  # å·²å­˜åœ¨ï¼Œæ‰§è¡Œäº†æ›´æ–°

    print(f"âœ… æˆåŠŸå°† {insert_count} æ¡äº‹ä»¶å†™å…¥ {settings.EVENT_NODE_COLLECTION}")


# ============================================================
# æ­¥éª¤ 7ï¼šå±•å¹³ç›®æ ‡èŠ‚ç‚¹ success.json â†’ TARGET_NODE_COLLECTION
# ============================================================

def flatten_target_nodes(db, collection_knowledge):
    """
    å±•å¹³æ¶‰åŠçš„ç›®æ ‡èŠ‚ç‚¹ä¿¡æ¯
    """
    new_collection = db[settings.TARGET_NODE_COLLECTION]

    if not os.path.exists(SUCCESS_JSON):
        return

    with open(SUCCESS_JSON, mode='r', encoding='utf-8') as f:
        records = json.load(f)

    # ä¸å†æ¸…ç©ºé›†åˆ
    target_ids = {r.get("target_id") for r in records if r.get("target_id")}

    insert_count = 0
    update_count = 0

    type_mapping = {
        "äººç‰©ç›®æ ‡": "äººç‰©", "ç»„ç»‡ç›®æ ‡": "ç»„ç»‡", "æœºåœºç›®æ ‡": "æœºåœº",
        "æ¸¯å£ç›®æ ‡": "æ¸¯å£", "å†›èˆ°ç›®æ ‡": "å†›èˆ°"
    }

    for tid in target_ids:
        doc = collection_knowledge.find_one({"target_id": tid})
        if not doc: continue

        data = doc.get("data", {}) or {}
        flat_data = flatten_dict(data)

        flat_data["target_id"] = tid
        flat_data["name"] = data.get("name", doc.get("name", ""))
        flat_data["source_url"] = doc.get("source_url", "")
        flat_data["event_first_level"] = doc.get("event_first_level", "")
        flat_data["event_second_level"] = doc.get("event_second_level", "")
        flat_data["type"] = type_mapping.get(flat_data["event_second_level"], "å…¶ä»–")

        result = new_collection.update_one(
            {"target_id": tid},  # ä»¥ target_id å”¯ä¸€å®šä½
            {"$set": flat_data},  # è¦†ç›–æ•°æ®
            upsert=True
        )

        if result.matched_count == 0:
            insert_count += 1
        else:
            update_count += 1

    print(f"âœ… ç›®æ ‡èŠ‚ç‚¹å±•å¹³å®Œæˆï¼šå†™å…¥ {insert_count} æ¡åˆ° {settings.TARGET_NODE_COLLECTION}")


# ============================================================
# æ€»å…¥å£
# ============================================================

def main():
    client = MongoClient(settings.MONGO_URI)
    db = client[settings.MONGO_DB_NAME]

    collection_interim = db[settings.INTERIM_COLLECTION]
    collection_knowledge = db[settings.KNOWLEDGE_COLLECTION_TARGET]

    # 1. æå– & æ¸…æ´—
    extract_and_clean_edges(collection_interim)

    # 2. å»é‡
    dedup_event_goal_edges_inplace()

    # 3. æ˜ å°„ knowledge_target
    map_event_goal_edges(collection_knowledge)

    # 4. æŠ½å–ç›®æ ‡å…³ç³»
    extract_target_target_edges(collection_knowledge)

    # 5. å…¥åº“è¾¹
    import_edges_to_mongo(db)

    # 6. å…¥åº“äº‹ä»¶èŠ‚ç‚¹
    extract_event_nodes_from_edges(db)

    # 7. å…¥åº“ç›®æ ‡èŠ‚ç‚¹
    flatten_target_nodes(db, collection_knowledge)


if __name__ == "__main__":
    main()