# semantic/Hot_topic/hotspot.py
# -*- coding: utf-8 -*-
"""
Hot Events -> Same-Doc Two Reports (classic & wiki)
ä¼˜åŒ–æœ€ç»ˆç‰ˆï¼š
1. [æ€§èƒ½] å¤ç”¨ Embedding æ¨¡å‹è¿›è¡Œå…³é”®è¯æå–ï¼Œé¿å…å¾ªç¯åŠ è½½ KeyBERTã€‚
2. [é€»è¾‘] æ”¯æŒå…¨é‡æ•°æ®æ‰«æï¼Œæ— è§†æ—¶é—´çª—å£ã€‚
3. [å­˜å‚¨] ç§»é™¤ç»˜å›¾å’Œæ–‡ä»¶æ“ä½œï¼ŒæŠ¥å‘Šå…¨æ–‡å­˜å…¥æ•°æ®åº“ã€‚
"""

import os
import re
import json
import math
import hashlib
import argparse
import datetime as dt
from collections import Counter
from urllib.parse import urlparse

import numpy as np
import pandas as pd
import requests

from pymongo import MongoClient
from bson import ObjectId
from dateutil import parser as dtparser

from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering

# å¼•å…¥ç»Ÿä¸€é…ç½®
from config.settings import settings

# ==========================================
# ã€ç¯å¢ƒé…ç½®ã€‘å¿…é¡»åœ¨å¯¼å…¥æ·±åº¦å­¦ä¹ åº“å‰è®¾ç½®
# ==========================================
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ---- å¯é€‰åº“ï¼ˆè‡ªåŠ¨é™çº§ï¼‰ ----
USE_HDBSCAN = True
try:
    import hdbscan
except Exception:
    USE_HDBSCAN = False

USE_KEYBERT = True
try:
    from keybert import KeyBERT
except Exception:
    USE_KEYBERT = False

# ------------------ å·¥å…·å‡½æ•° ------------------
SEP = "ã€‚"
SAFEWORDS = {"é‡ç£…", "éœ‡æ’¼", "å²æ— å‰ä¾‹", "æƒ¨", "æ€’", "æƒŠå¤©", "æ ¸æ‰“å‡»", "æ­¼ç­", "è¡€æ´—"}
SENT_SPLIT_RE = re.compile(r"[ã€‚ï¼ï¼Ÿï¼›;\n]+")
DATE_RE = re.compile(r"\d{4}-\d{2}-\d{2}")


def norm_text(s: str) -> str:
    if not s: return ""
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"http[s]?://\S+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def concat_for_embed(title, content, max_chars=800):
    t = norm_text(title or "")
    c = norm_text(content or "")
    if len(c) > max_chars: c = c[:max_chars]
    return (t + SEP + c).strip()


def parse_date_any(x):
    if x is None: return dt.datetime.utcnow()
    if isinstance(x, dt.datetime): return x
    if isinstance(x, str):
        try:
            return dtparser.parse(x)
        except Exception:
            pass
    return dt.datetime.utcnow()


def to_datestr(d):
    if isinstance(d, dt.datetime): d = d.date()
    return d.strftime("%Y-%m-%d")


def recency_score(latest_date: dt.date, today: dt.date) -> float:
    dd = max(0, (today - latest_date).days)
    return math.exp(- dd / settings.HOTSPOT_RECENCY_TAU_DAYS)


def diversity_ratio(sources: list, total: int) -> float:
    if total == 0: return 0.0
    return len(set([s for s in sources if s])) / total


def stable_event_id(news_ids: list) -> str:
    m = hashlib.md5()
    for x in sorted(news_ids):
        m.update(x.encode("utf-8"))
    return m.hexdigest()


def choose_event_time(dates: list) -> dict:
    if not dates: return {"type": "unknown", "value": None}
    dmin, dmax = min(dates), max(dates)
    if dmin == dmax:
        return {"type": "point", "value": to_datestr(dmin)}
    if (dmax - dmin).days <= 3:
        cnt = Counter(dates)
        peak = max(cnt.items(), key=lambda x: x[1])[0]
        return {"type": "point", "value": to_datestr(peak)}
    return {"type": "range", "value": [to_datestr(dmin), to_datestr(dmax)]}


def derive_source_from_url(url: str) -> str:
    try:
        host = urlparse(url).netloc.lower()
        return host[4:] if host.startswith("www.") else host
    except Exception:
        return ""


def parse_date_from_text(text: str):
    if not text: return None
    s = text.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", " ").replace("æ—¶é—´ï¼š", " ")
    s = s.replace("æ™‚", " ").replace("ï¼š", ":")
    m = re.search(r"(\d{4})[-/](\d{1,2})[-/](\d{1,2})(?:\s+(\d{1,2}:\d{2}:\d{2}))?", s)
    if m:
        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
        t = m.group(4) or "00:00:00"
        try:
            return dt.datetime.strptime(f"{y:04d}-{mo:02d}-{d:02d} {t}", "%Y-%m-%d %H:%M:%S")
        except Exception:
            return None
    return None


# -------- åµŒå…¥ä¸èšç±» --------
def embed_docs(model: SentenceTransformer, docs):
    texts = [concat_for_embed(d.get("title", ""), d.get("content", "")) for d in docs]
    # batch_size å¯æ ¹æ®æ˜¾å­˜è°ƒæ•´
    emb = model.encode(texts, normalize_embeddings=True, batch_size=64, show_progress_bar=True)
    return np.asarray(emb, dtype=np.float32)


def cluster_embeddings(emb: np.ndarray):
    if len(emb) == 0: return np.array([])
    # é™ç»´å¤„ç†
    n_components = min(50, emb.shape[1])
    if len(emb) > n_components:
        X = PCA(n_components=n_components).fit_transform(emb)
    else:
        X = emb

    min_cluster_size = settings.HOTSPOT_MIN_CLUSTER_SIZE

    if USE_HDBSCAN and len(emb) >= min_cluster_size * 2:
        cl = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=2, metric="euclidean")
        labels = cl.fit_predict(X)
    else:
        k = max(2, len(emb) // max(2, min_cluster_size * 2))
        cl = AgglomerativeClustering(n_clusters=k)
        labels = cl.fit_predict(X)
    return labels


# -------- å…³é”®è¯ä¸æ ‡é¢˜ (ä¼˜åŒ–ç‰ˆ) --------
try:
    import jieba

    HAS_JIEBA = True
except Exception:
    HAS_JIEBA = False


def _zh_tokenize(s: str):
    toks = [t.strip() for t in jieba.lcut(s, cut_all=False) if t.strip()]
    toks = [t for t in toks if 2 <= len(t) <= 8]
    return toks


# ã€ä¼˜åŒ–ç‚¹ã€‘å¢åŠ  model å‚æ•°ï¼Œå¤ç”¨å·²åŠ è½½çš„æ¨¡å‹
def extract_keywords(texts: list, model=None, topk=5) -> list:
    texts = [t for t in texts if t]
    if not texts: return []

    if USE_KEYBERT:
        try:
            # å¦‚æœä¼ å…¥äº†é¢„åŠ è½½çš„ model (SentenceTransformer å¯¹è±¡)ï¼Œç›´æ¥å¤ç”¨
            if model:
                kb = KeyBERT(model=model)
            else:
                # å¦åˆ™é‡æ–°åŠ è½½ (æ…¢)
                kb = KeyBERT(model=settings.HOTSPOT_EMB_MODEL)

            joined = "ã€‚".join(texts)
            # æå–å…³é”®è¯
            cands = kb.extract_keywords(joined, top_n=max(topk, 8), keyphrase_ngram_range=(1, 3), stop_words=None)
            out = [re.sub(r"[^\u4e00-\u9fa5A-Za-z0-9]+", "", w).strip() for w, _ in cands]
            out = [w for w in out if 2 <= len(w) <= 10]
            if out: return out[:topk]
        except Exception:
            pass

    # é™çº§æ–¹æ¡ˆï¼šTF-IDF
    if HAS_JIEBA:
        vec = TfidfVectorizer(tokenizer=_zh_tokenize, token_pattern=None, max_features=8000, ngram_range=(1, 2))
    else:
        vec = TfidfVectorizer(analyzer="char", ngram_range=(2, 4), max_features=8000)

    try:
        X = vec.fit_transform(texts + texts)
    except ValueError:
        return []

    scores = np.asarray(X.sum(0)).ravel()
    vocab = np.array(vec.get_feature_names_out())
    idx = scores.argsort()[::-1]
    kws = []
    for i in idx:
        k = vocab[i]
        k = re.sub(r"[^\u4e00-\u9fa5A-Za-z0-9]+", "", k).strip()
        if 2 <= len(k) <= 10:
            kws.append(k)
        if len(kws) >= topk: break
    return kws


def clean_keywords(kws: list, max_len_each=8, max_k=5) -> list:
    seen, out = set(), []
    for k in kws:
        k = re.sub(r"[^\u4e00-\u9fa5A-Za-z0-9]+", "", (k or "")).strip()
        if not k: continue
        if len(k) > max_len_each: k = k[:max_len_each]
        if 2 <= len(k) and k not in seen:
            seen.add(k);
            out.append(k)
        if len(out) >= max_k: break
    return out


def normalize_title(t: str) -> str:
    t = (t or "").strip().replace("  ", " ")
    t = re.sub(r"[ï¼Œ,]\s*", "ï¼Œ", t).strip("ã€‚!ï¼?ï¼Ÿ~ã€ ")
    t = t.replace("â€œ", "ã€Œ").replace("â€", "ã€")
    return t


def enforce_title_len(title: str, key_terms: list, max_len=20) -> str:
    t = (title or "").strip()
    if not t or len(t) <= max_len: return t
    parts = [p for p in re.split(r"[ï¼Œ,ã€:ï¼š;ï¼›\-â€”\s]+", t) if p]
    if parts:
        scored = []
        for p in parts:
            cov = sum(1 for k in key_terms if k and k in p)
            scored.append((cov, -len(p), p))
        scored.sort(reverse=True)
        acc, cur = [], 0
        for _, __, p in scored:
            add = (1 if acc else 0) + len(p)
            if cur + add <= max_len:
                acc.append(p);
                cur += add
        if acc:
            t2 = "ã€".join(acc)
            if len(t2) <= max_len: return t2
    t = t[:max_len]
    return t.rstrip("ï¼Œã€:ï¼š;ï¼›-â€” ")


def title_score(t: str, key_terms: list, entities: list, max_len=20) -> float:
    if not t: return -1e9
    tt = normalize_title(t)
    if any(w in tt for w in SAFEWORDS): return -1e6
    L = len(tt)
    length_pen = -1.0 * max(0, L - max_len)
    cov = sum(1 for k in key_terms if k and k in tt)
    ent = 1 if any(e for e in entities if e and e in tt) else 0
    action = 1 if re.search(r"(æ¼”ä¹ |ä¼šè§|è®¿å°|å¯¹å³™|åˆ¶è£|è¡¨å†³|é€šè¿‡|çˆ†å‘|å æ¯|é›†ä¼š|æŠ—è®®|è°ˆåˆ¤|åˆä½œ|ç­¾ç½²|å¯åŠ¨|é€šæŠ¥|æœæ•‘|åœç«|å†²çª)", tt) else 0
    return 2.0 * cov + 1.2 * ent + 0.8 * action + length_pen


def pick_best_title(cands: list, key_terms: list, entities: list, max_len=20) -> str:
    seen, scored = set(), []
    for t in cands or []:
        t = normalize_title(t)
        if not t or t in seen: continue
        seen.add(t)
        if len(t) > 2 * max_len: continue
        scored.append((title_score(t, key_terms, entities, max_len), t))
    if not scored: return ""
    scored.sort(reverse=True)
    return scored[0][1]


def ollama_generate_json(prompt: str, timeout=240) -> dict:
    url = settings.OLLAMA_HOST.rstrip("/") + "/api/generate"
    try:
        resp = requests.post(url, json={
            "model": settings.HOTSPOT_OLLAMA_MODEL,
            "prompt": prompt,
            "stream": True,
            "options": {"temperature": 0.2, "top_p": 0.95}
        }, stream=True, timeout=timeout)
        resp.raise_for_status()
        buf = ""
        for line in resp.iter_lines(decode_unicode=True):
            if not line: continue
            try:
                obj = json.loads(line);
                buf += obj.get("response", "")
                if obj.get("done"): break
            except Exception:
                continue
        m = re.search(r"\{[\s\S]*\}", buf)
        if not m: return {}
        return json.loads(m.group(0))
    except Exception:
        return {}


LLM_PROMPT_TITLE = """ä½ æ˜¯â€œæ–°é—»äº‹ä»¶å‘½åå™¨â€ã€‚æ ¹æ®ä¸‹åˆ—ä»£è¡¨æ€§æ ‡é¢˜ã€å…³é”®è¯ä¸æ—¶é—´èŒƒå›´ï¼Œç”Ÿæˆ5-8ä¸ªä¸­æ–‡å€™é€‰æ ‡é¢˜ï¼ˆä¸­æ€§ã€ä¿¡æ¯å¯†åº¦é«˜ã€â‰¤20å­—ã€æ— å¥å·/æ„Ÿå¹å·ï¼‰ï¼Œåªè¾“å‡ºJSONï¼š
{{"titles":[...], "keywords_suggested":[...]}}
ä»£è¡¨æ€§æ ‡é¢˜ï¼š
{titles}
å…³é”®è¯ï¼ˆé«˜â†’ä½ï¼‰ï¼š{key_terms}
å®ä½“ï¼š{entities}
æ—¶é—´èŒƒå›´ï¼š{time_range}
"""


def make_event_title_llm(top_titles, key_terms, entities, time_range, fallback_func, max_len=20):
    prompt = LLM_PROMPT_TITLE.format(
        titles="\n".join("- " + t for t in top_titles[:10]),
        key_terms=", ".join(key_terms[:12]),
        entities=", ".join(entities[:10]),
        time_range=time_range or ""
    )
    titles, kws_llm = [], []
    try:
        obj = ollama_generate_json(prompt)
        titles = obj.get("titles") or []
        kws_llm = obj.get("keywords_suggested") or []
    except Exception:
        pass
    best = pick_best_title(titles, key_terms, entities, max_len=max_len) if titles else ""
    if not best:
        best = make_event_title_fallback(key_terms, top_titles)
        kws_llm = []
    return best, kws_llm


def make_event_title_fallback(kws: list, top_titles: list) -> str:
    kws = clean_keywords(kws, max_len_each=8, max_k=5)
    if kws:
        if len(kws) >= 3: return f"{kws[0]}ã€{kws[1]}ä¸{kws[2]}"
        if len(kws) == 2: return f"{kws[0]}ä¸{kws[1]}"
        return kws[0]
    if not top_titles: return "çƒ­ç‚¹äº‹ä»¶"
    s1, s2 = min(top_titles, key=len), max(top_titles, key=len)
    prefix = ""
    for i, ch in enumerate(s1):
        if s2[i:i + 1] != ch: prefix = s1[:i]; break
    else:
        prefix = s1
    prefix = prefix.strip(" -ï¼Œã€‚ã€ã€Šã€‹")
    if len(prefix) < 4: prefix = (top_titles[0][:16] if top_titles else "çƒ­ç‚¹äº‹ä»¶")
    return prefix


# ------------------ äº‹å®åº“ä¸æ ¡éªŒ ------------------
def build_fact_bank(news, keywords, max_sentences=120):
    kwset = set(keywords or [])
    sents, refs, dates = [], [], set()
    for d in news:
        url = (d.get("url") or "").strip()
        if url: refs.append(url)
        try:
            dates.add(to_datestr(parse_date_any(d.get("published_at"))))
        except Exception:
            pass

        chunks = []
        t = (d.get("title") or "").strip()
        c = (d.get("content") or "").strip()
        if t: chunks.append(t)
        if c:
            chunks.extend([x.strip() for x in SENT_SPLIT_RE.split(c) if x and 8 <= len(x) <= 120])

        for s in chunks:
            hit = sum(1 for k in kwset if k and k in s)
            has_num = 1 if re.search(r"\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}", s) or re.search(r"\d+", s) else 0
            score = 2 * hit + has_num
            if score > 0:
                sents.append((score, s))

    sents.sort(key=lambda x: (-x[0], -len(x[1])))
    seen = set();
    picked = []
    for _, s in sents:
        ss = re.sub(r"\s+", "", s)
        if ss in seen: continue
        seen.add(ss)
        picked.append(s)
        if len(picked) >= max_sentences: break

    fact_lines = [f"- {s}" for s in picked]
    fact_bank_text = "\n".join(fact_lines[:max_sentences])
    references = sorted(set([u for u in refs if u]))
    allowed_dates = sorted(set(dates))
    return fact_bank_text, references, allowed_dates


def filter_progression_by_dates(prog_list, allowed_dates, max_items=8):
    out = []
    allowset = set(allowed_dates or [])
    for item in prog_list or []:
        d = (item.get("date") or "").strip()
        txt = f"{d} {item.get('what', '')}"
        ds = set(DATE_RE.findall(txt))
        if not ds or ds.issubset(allowset):
            out.append(item)
        if len(out) >= max_items: break
    return out


# ------------------ æŠ¥å‘Š Prompt ------------------
PROMPT_REPORT_CLASSIC = """ä½ æ˜¯â€œæ–°é—»ä¸“é¢˜æŠ¥å‘Šæ’°ç¨¿äººâ€ã€‚ä»…åŸºäºã€äº‹å®åº“ã€‘æ”¹å†™ï¼Œç¦æ­¢æ·»åŠ äº‹å®åº“ä¹‹å¤–çš„å…·ä½“æ—¶é—´ã€æ•°å­—ã€äººåã€æœºæ„åã€‚
å¦‚ç´ æä¸è¶³è¯·å†™â€œï¼ˆç´ æä¸è¶³ï¼‰â€ï¼Œä¸è¦è‡ªè¡Œè¡¥å……ã€‚åªè¾“å‡ºåˆæ³• JSONã€‚
ã€äº‹å®åº“ã€‘
%(fact_bank)s

{
  "overall_summary": "150-250å­—æ•´ä½“æ‘˜è¦ï¼ˆä¸­æ€§ï¼Œå«å…³é”®ç»“è®º+ç›´æ¥è¯æ®ï¼‰",
  "background": "500-800å­—èƒŒæ™¯/å‰å²/äº‰ç‚¹ï¼ˆè¯´æ˜ä¸æœ¬æ¬¡æ—¶é—´èŒƒå›´å…³ç³»ï¼‰",
  "progression": [
    {"date":"YYYY-MM-DD","title":"<=20å­—","what":"40-80å­—","evidence":["è¦ç‚¹1","è¦ç‚¹2"]}
  ],
  "impacts": {
    "politics": ["3-6æ¡ï¼ˆ<=40å­—/æ¡ï¼‰"],
    "diplomacy": ["..."],
    "military": ["..."],
    "economy": ["..."],
    "public_opinion": ["..."]
  },
  "detection_status": {
    "first_article_date": "%(first_article_date)s",
    "event_created_at": "%(event_created_at)s",
    "detection_latency_days": %(detection_latency_days)d,
    "cluster_size": %(cluster_size)d,
    "unique_sources": %(unique_sources)d,
    "dup_title_ratio": %(dup_title_ratio).3f,
    "method": "HDBSCAN/Agglomerative",
    "params": {"min_cluster_size": %(min_cluster_size)d}
  },
  "key_targets": [],
  "evolution": {"phases": [], "explanation":"ï¼ˆå¦‚å¯ç”¨æ—¶å¡«å†™ï¼‰"},
  "forecast": {"horizon_days": %(horizon)d, "volume_forecast": [], "scenarios": [], "caveats":""},
  "references": []
}
ç¡¬æ€§è§„åˆ™ï¼šä¸­æ€§ã€æ— æ„Ÿå¹å·ï¼›progression â‰¥3ï¼›ä»…è¾“å‡ºåˆæ³• JSONã€‚
ã€äº‹ä»¶ã€‘%(event_title)s
ã€æ—¶é—´èŒƒå›´ã€‘%(time_range)s
ã€å…³é”®è¯ã€‘%(keywords)s
ã€ä»£è¡¨æ€§æ ‡é¢˜ï¼ˆâ‰¤12æ¡ï¼‰ã€‘
%(top_titles)s
ã€ç»Ÿè®¡ã€‘æ€»é‡=%(news_count)dï¼›æ¥æºæ•°=%(source_cnt)dï¼›æ—¥å‡=%(daily_avg).2fï¼›å³°å€¼æ—¥=%(peak_date)s(%(peak_count)d)
"""

PROMPT_REPORT_WIKI = """ä½ æ˜¯â€œç™¾ç§‘ä½“ä¸“é¢˜æ’°ç¨¿äººâ€ã€‚ä»…åŸºäºã€äº‹å®åº“ã€‘æ”¹å†™ï¼Œç¦æ­¢æ·»åŠ äº‹å®åº“ä¹‹å¤–çš„å…·ä½“æ—¶é—´ã€æ•°å­—ã€äººåã€æœºæ„åã€‚
å¦‚ç´ æä¸è¶³è¯·å†™â€œï¼ˆç´ æä¸è¶³ï¼‰â€ï¼Œä¸è¦è‡ªè¡Œè¡¥å……ã€‚åªè¾“å‡ºåˆæ³• JSONã€‚
ã€äº‹å®åº“ã€‘
%(fact_bank)s

{
  "lead": "120-200å­—å¯¼è¯­/åºè¨€ï¼ˆä¸­æ€§å®¢è§‚ã€ä¿¡æ¯å¯†é›†ï¼‰",
  "background_and_precedents": {
    "congress_taiwan_relations": "200-350å­—ï¼›å›½ä¼šä¸å°æ¹¾å…³ç³»è¦ç‚¹ï¼ˆå«å…³é”®æ³•æ¡ˆ/è®¿é—®å…ˆä¾‹ï¼‰",
    "pelosi_taiwan_relations": "150-300å­—ï¼›è£´æ´›è¥¿ä¸å°æ¹¾çš„å†å²å…³è”"
  },
  "pretrip_and_preparations": "250-400å­—ï¼›é‚€è®¿ç£‹å•†ä¸è¡Œå‰æ¶ˆæ¯ï¼ˆæ—¶é—´çº¿è¦ç´ +æ¥æºè¦ç‚¹ï¼‰",
  "personnel": ["è®¿é—®å›¢ä¸ä¸»è¦æ¥å¾…å®˜å‘˜ï¼ˆè‹¥å¯å½’çº³ï¼›æ¯æ¡â‰¤30å­—ï¼‰"],
  "itinerary": {
    "before_arrival": [{"date":"YYYY-MM-DD","event":"<=20å­—","detail":"40-80å­—"}],
    "arrival_night":  [{"date":"YYYY-MM-DD","event":"<=20å­—","detail":"40-80å­—"}],
    "next_day":       [{"date":"YYYY-MM-DD","event":"<=20å­—","detail":"40-80å­—"}],
    "departure":      [{"date":"YYYY-MM-DD","event":"<=20å­—","detail":"40-80å­—"}],
    "after":          [{"date":"YYYY-MM-DD","event":"<=20å­—","detail":"40-80å­—"}]
  },
  "related_military_actions": ["3-6æ¡ï¼›ç›¸å…³å†›äº‹è¡ŒåŠ¨ä¸æ—¶é—´ç‚¹ï¼ˆâ‰¤40å­—/æ¡ï¼‰"],
  "reactions": {"PRC": [], "Taiwan": [], "US": [], "Others": []},
  "related_events_and_impacts": {"cyberattacks": [], "sanctions": [], "trade": [], "follow_on_visits": []},
  "detection_status": {
    "first_article_date": "%(first_article_date)s",
    "event_created_at": "%(event_created_at)s",
    "detection_latency_days": %(detection_latency_days)d,
    "cluster_size": %(cluster_size)d,
    "unique_sources": %(unique_sources)d,
    "dup_title_ratio": %(dup_title_ratio).3f,
    "method": "HDBSCAN/Agglomerative",
    "params": {"min_cluster_size": %(min_cluster_size)d}
  },
  "key_targets": [],
  "evolution": {"phases": [], "explanation": ""},
  "forecast": {"horizon_days": 0, "volume_forecast": [], "scenarios": [], "caveats": ""},
  "references": []
}
è¦æ±‚ï¼šé¿å…å¤¸å¼ å’Œç«‹åœºæ€§æªè¾ï¼›å°½é‡åŸºäºä»£è¡¨æ€§æ ‡é¢˜ä¸­çš„æ˜ç¡®äº‹å®ã€‚
ã€äº‹ä»¶ã€‘%(event_title)s
ã€æ—¶é—´èŒƒå›´ã€‘%(time_range)s
ã€å…³é”®è¯ã€‘%(keywords)s
ã€ä»£è¡¨æ€§æ ‡é¢˜ï¼ˆâ‰¤12æ¡ï¼‰ã€‘
%(top_titles)s
ã€ç»Ÿè®¡ã€‘æ€»é‡=%(news_count)dï¼›æ¥æºæ•°=%(source_cnt)dï¼›æ—¥å‡=%(daily_avg).2fï¼›å³°å€¼æ—¥=%(peak_date)s(%(peak_count)d)
"""


# ------------------ ä¸»ç±» ------------------
class HotEventsTwoReportsStrict:
    def __init__(self, source_collections=None):
        self.client = MongoClient(settings.MONGO_URI)
        self.db = self.client[settings.MONGO_DB_NAME]

        # ä¼˜åŒ–ï¼šå…è®¸ä¼ å…¥æ‰€æœ‰æºé›†åˆåç§°ï¼Œè‹¥ä¸ä¼ åˆ™ä»é…ç½®è¯»å–
        self.source_collections = source_collections if source_collections else settings.COL_SRC_LIST
        self.col_out = self.db[settings.KNOWLEDGE_COLLECTION_EVENT]

        # ç§»é™¤æœ¬åœ°ç›®å½•åˆ›å»º
        # os.makedirs(settings.HOTSPOT_OUT_DIR, exist_ok=True)

        if settings.HTTP_PROXY:
            os.environ["HTTP_PROXY"] = settings.HTTP_PROXY
            os.environ["HTTPS_PROXY"] = settings.HTTP_PROXY

        print(f"[Hotspot] Loading Embedding Model: {settings.HOTSPOT_EMB_MODEL}")
        self.embedder = SentenceTransformer(settings.HOTSPOT_EMB_MODEL)

    # --- ä¼˜åŒ–æ ¸å¿ƒï¼šå¤šæºåŠ è½½ (å…¨é‡æ‰«ææ¨¡å¼) ---
    def load_recent_docs(self, days_window=None, limit_per_col=None):
        """
        åŠ è½½æ–‡æ¡£ç”¨äºèšç±»ã€‚
        ä¿®æ”¹è¯´æ˜ï¼šç§»é™¤æ—¶é—´çª—å£é™åˆ¶ï¼Œè¿›è¡Œå…¨é‡æ‰«æã€‚
        """
        # å¦‚æœæœªæŒ‡å®šï¼Œç»™äºˆä¸€ä¸ªè¾ƒå¤§çš„é»˜è®¤å€¼ï¼Œç¡®ä¿èƒ½è¦†ç›–ç°æœ‰æ•°æ®é‡
        if limit_per_col is None:
            # å°Šé‡ settings ä¸­çš„é…ç½®ï¼Œå¦‚æœæ²¡é…åˆ™é»˜è®¤ 5000
            limit_per_col = getattr(settings, 'HOTSPOT_BATCH_LIMIT', 5000)

            # æ‰“å°ä¸€ä¸‹å®é™…ä½¿ç”¨çš„é™åˆ¶
        print(f"[Hotspot] æ•°æ®åŠ è½½é™åˆ¶: æ¯æº {limit_per_col} æ¡")

        proj = {"title": 1, "content": 1, "url": 1, "published_at": 1, "scraped_at": 1, "source": 1}


        all_docs = []

        for col_name in self.source_collections:
            try:
                col = self.db[col_name]
                # ã€å…¨é‡æ‰«æã€‘queryä¸ºç©ºï¼ŒåŒ¹é…æ‰€æœ‰æ•°æ®
                query = {}
                # æŒ‰å‘å¸ƒæ—¶é—´å€’åºå–æœ€æ–°çš„ limit æ¡
                docs = list(col.find(query, proj).sort("published_at", -1).limit(limit_per_col))

                for d in docs:
                    d["_id_str"] = str(d["_id"])
                    pub = d.get("published_at") or parse_date_from_text(d.get("content", "")) or d.get("scraped_at")
                    d["published_at"] = parse_date_any(pub)

                    if not d.get("source"):
                        if "cctv" in col_name.lower():
                            d["source"] = "CCTV"
                        elif "toutiao" in col_name.lower():
                            d["source"] = "å¤´æ¡"
                        elif "weibo" in col_name.lower():
                            d["source"] = "å¾®åš"
                        elif "xinhua" in col_name.lower():
                            d["source"] = "æ–°åç½‘"
                        elif "cnn" in col_name.lower():
                            d["source"] = "CNN"
                        else:
                            d["source"] = derive_source_from_url(d.get("url", ""))

                    all_docs.append(d)
            except Exception as e:
                print(f"[Hotspot] Warning: Load from {col_name} failed: {e}")

        print(f"[Hotspot] èåˆäº† {len(self.source_collections)} ä¸ªæº, å…±åŠ è½½ {len(all_docs)} æ¡æ–°é—» (æ¨¡å¼: å…¨é‡/æ— æ—¶é—´çª—å£)")
        return all_docs

    def build_events(self, docs):
        if not docs: return []
        emb = embed_docs(self.embedder, docs)
        labels = cluster_embeddings(emb)
        today = dt.datetime.utcnow().date()
        events = []

        max_title_len = settings.HOTSPOT_MAX_TITLE_LEN
        min_cluster_size = settings.HOTSPOT_MIN_CLUSTER_SIZE

        for lb in sorted(set(labels)):
            if lb == -1: continue
            idx = np.where(labels == lb)[0].tolist()
            if len(idx) < min_cluster_size: continue
            group = [docs[i] for i in idx]
            titles = [g.get("title", "") for g in group]
            bodies = [concat_for_embed(g.get("title", ""), g.get("content", "")) for g in group]

            # ã€ä¼˜åŒ–ç‚¹ã€‘ä¼ å…¥ self.embedderï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹
            kws_raw = extract_keywords(titles + bodies, model=self.embedder, topk=5)

            kws = clean_keywords(kws_raw, max_len_each=8, max_k=5)

            dates = [g.get("published_at", dt.datetime.utcnow()).date() for g in group]
            ev_time = choose_event_time(dates)
            time_range_str = ev_time["value"] if ev_time["type"] == "point" else " è‡³ ".join(ev_time["value"]) if \
            ev_time["value"] else ""

            entities = kws[:3]
            title_llm, kws_llm = make_event_title_llm(
                top_titles=titles[:8], key_terms=kws, entities=entities, time_range=time_range_str,
                fallback_func=make_event_title_fallback, max_len=max_title_len
            )
            final_kws = clean_keywords(kws_llm or kws, max_len_each=8, max_k=5)
            final_title = enforce_title_len(title_llm or make_event_title_fallback(final_kws, titles[:8]), final_kws,
                                            max_len=max_title_len)

            news_ids = [g["_id_str"] for g in group]
            sources = [g.get("source", "") for g in group]
            latest_date = max(dates)
            ncount = len(group)
            rec = recency_score(latest_date, today)
            div = diversity_ratio(sources, ncount)
            score = round(settings.HOTSPOT_ALPHA * math.log1p(
                ncount) + settings.HOTSPOT_BETA * rec + settings.HOTSPOT_GAMMA * div, 4)
            eid = stable_event_id(news_ids)

            events.append({
                "event_id": eid,
                "event_title": final_title,
                "keywords": final_kws,
                "news_count": ncount,
                "score": score,
                "news_ids": news_ids,
                "search_keywords": " ".join(final_kws),
                "event_time": ev_time,
                "created_at": dt.datetime.utcnow(),
                "updated_at": dt.datetime.utcnow(),
            })
        return events

    def upsert_event(self, ev):
        existed = self.col_out.find_one({"event_id": ev["event_id"]}, {"_id": 1})
        if existed:
            self.col_out.update_one({"event_id": ev["event_id"]}, {"$set": ev})
        else:
            self.col_out.insert_one(ev)

    def _collect_news_stats(self, ev):
        """
        ã€ä¿®æ”¹ç‰ˆã€‘åªæ”¶é›†ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸ç»˜å›¾ï¼Œä¸ç”Ÿæˆæ–‡ä»¶è·¯å¾„
        """
        news = []
        ids_to_find = set(ev["news_ids"])

        for col_name in self.source_collections:
            if not ids_to_find: break
            q_ids = []
            for x in ids_to_find:
                try:
                    q_ids.append(ObjectId(x))
                except:
                    pass

            col = self.db[col_name]
            found = list(col.find({"_id": {"$in": q_ids}},
                                  {"title": 1, "content": 1, "published_at": 1, "scraped_at": 1, "source": 1,
                                   "url": 1}))
            for d in found:
                ids_to_find.remove(str(d["_id"]))
                pub = d.get("published_at") or parse_date_from_text(d.get("content", "")) or d.get("scraped_at")
                d["published_at"] = parse_date_any(pub)
                if not d.get("source"): d["source"] = derive_source_from_url(d.get("url", ""))
                news.append(d)

        news.sort(key=lambda x: x["published_at"])

        dates = [to_datestr(n["published_at"]) for n in news]
        sources = [n.get("source", "") for n in news]
        titles = [(n.get("title") or "").strip() for n in news]
        norm_titles = [re.sub(r"\s+", " ", t) for t in titles if t]
        dup_ratio = 1.0 - len(set(norm_titles)) / len(norm_titles) if norm_titles else 0.0

        # --- ã€ä¿®æ”¹ç‚¹ã€‘ç§»é™¤æ‰€æœ‰ç»˜å›¾é€»è¾‘ ---
        # ä»…ä¿ç•™æ•°æ®è®¡ç®—ç”¨äºæŠ¥å‘Šç”Ÿæˆ
        dates_dt = pd.to_datetime(pd.Series(dates)).dt.date
        daily_cnt = pd.Series(1, index=dates_dt).groupby(level=0).sum().sort_index()

        news_count = len(news)
        source_cnt = len(set([s for s in sources if s]))
        daily_avg = news_count / max(1, len(daily_cnt)) if len(daily_cnt) > 0 else 0.0
        peak_date, peak_count = ("â€”", 0)
        if len(daily_cnt) > 0:
            peak_date = to_datestr(daily_cnt.idxmax());
            peak_count = int(daily_cnt.max())

        et = ev.get("event_time", {})
        if et.get("type") == "point":
            time_range = et.get("value")
        elif et.get("type") == "range" and et.get("value"):
            time_range = f"{et['value'][0]} è‡³ {et['value'][1]}"
        else:
            time_range = "æœªçŸ¥"

        first_article_date = to_datestr(news[0]["published_at"]) if news else "æœªçŸ¥"
        event_created_at = to_datestr(parse_date_any(ev.get("created_at"))) if ev.get("created_at") else "æœªçŸ¥"
        try:
            _d1 = parse_date_any(ev.get("created_at")).date()
            _d0 = parse_date_any(news[0]["published_at"]).date() if news else _d1
            detection_latency_days = max(0, (_d1 - _d0).days)
        except Exception:
            detection_latency_days = 0

        titles_clean = []
        for d in news[-200:]:
            t = (d.get("title") or "").strip()
            if 6 <= len(t) <= 80:
                titles_clean.append(f"{to_datestr(d['published_at'])}ï½œ{d.get('source', '')}ï½œ{t}")
        top_titles = "\n".join("- " + x for x in titles_clean[-12:])
        keywords_line = ", ".join(ev.get("keywords", [])[:12])

        horizon = 7

        base_fmt = dict(
            event_title=ev.get("event_title", "çƒ­ç‚¹äº‹ä»¶"),
            time_range=time_range,
            keywords=keywords_line or "ï¼ˆæš‚æ— ï¼‰",
            top_titles=top_titles or "ï¼ˆä»£è¡¨æ€§æ ‡é¢˜ä¸è¶³ï¼‰",
            news_count=news_count,
            source_cnt=source_cnt,
            daily_avg=daily_avg,
            peak_date=peak_date, peak_count=peak_count,
            first_article_date=first_article_date,
            event_created_at=event_created_at,
            detection_latency_days=detection_latency_days,
            cluster_size=news_count,
            unique_sources=source_cnt,
            dup_title_ratio=dup_ratio,
            min_cluster_size=settings.HOTSPOT_MIN_CLUSTER_SIZE,
            horizon=horizon
        )

        # ç»Ÿè®¡å¯¹è±¡ä¿ç•™ï¼Œä¾›åç»­å¯èƒ½ä½¿ç”¨
        stats_obj = {
            "daily_counts": {to_datestr(k): int(v) for k, v in daily_cnt.items()},
            "news_count": news_count,
            "source_count": source_cnt,
            "dup_title_ratio": dup_ratio,
            "peak_date": peak_date, "peak_count": peak_count,
        }
        return news, base_fmt, stats_obj

    # === [æ–°å¢] JSON è½¬ Markdown è¾…åŠ©æ–¹æ³• ===
    def _json_to_md_classic(self, title, data):
        md = []
        md.append(f"# {title}")
        if data.get("overall_summary"):
            md.append(f"## æ‘˜è¦\n{data['overall_summary']}")
        if data.get("background"):
            md.append(f"## èƒŒæ™¯ä¸å‰å²\n{data['background']}")
        if data.get("progression"):
            md.append("## äº‹ä»¶è¿›å±•")
            for item in data["progression"]:
                md.append(f"- **{item.get('date', '')}**: {item.get('what', '')}")
        if data.get("impacts"):
            md.append("## å½±å“è¯„ä¼°")
            for k, v in data["impacts"].items():
                if v:
                    lines = "\n".join([f"  - {x}" for x in v])
                    md.append(f"- **{k}**:\n{lines}")
        return "\n\n".join(md)

    def _json_to_md_wiki(self, title, data):
        md = []
        md.append(f"# {title}")
        if data.get("lead"):
            md.append(f"## å¯¼è¯­\n{data['lead']}")
        if data.get("background_and_precedents"):
            md.append("## èƒŒæ™¯")
            for k, v in data["background_and_precedents"].items():
                md.append(f"### {k}\n{v}")
        if data.get("itinerary"):
            md.append("## æ—¶é—´çº¿")
            for stage, events in data["itinerary"].items():
                md.append(f"### {stage}")
                for e in events:
                    md.append(f"- **{e.get('date')}**: {e.get('event')} ({e.get('detail')})")
        return "\n\n".join(md)

    def _gen_one_style(self, ev, base_fmt, stats_obj, references, allowed_dates, style: str):
        prompt = (PROMPT_REPORT_WIKI % base_fmt) if style == "wiki" else (PROMPT_REPORT_CLASSIC % base_fmt)
        try:
            sections = ollama_generate_json(prompt)
        except Exception:
            # å›é€€é€»è¾‘ (ç®€åŒ–ç‰ˆ)
            sections = {}

            # è¿‡æ»¤å¼€å…³
        if settings.HOTSPOT_DISABLE_KEY_TARGETS and "key_targets" in sections: sections["key_targets"] = []
        if settings.HOTSPOT_DISABLE_EVOLUTION and "evolution" in sections: sections["evolution"] = {"phases": [],
                                                                                                    "explanation": ""}
        if settings.HOTSPOT_DISABLE_FORECAST and "forecast" in sections: sections["forecast"] = {"horizon_days": 0,
                                                                                                 "volume_forecast": [],
                                                                                                 "scenarios": [],
                                                                                                 "caveats": ""}

        if "progression" in sections:
            sections["progression"] = filter_progression_by_dates(sections.get("progression") or [], allowed_dates)

        try:
            if isinstance(sections, dict):
                if "references" in sections and isinstance(sections["references"], list):
                    sections["references"] = sorted(set(sections["references"] + references))
                else:
                    sections["references"] = references
        except Exception:
            pass

        # === [æ ¸å¿ƒä¿®æ”¹] ç”Ÿæˆå…¨æ–‡ Markdownï¼Œä¸å†™å…¥æœ¬åœ°æ–‡ä»¶ ===
        if style == 'wiki':
            content_md = self._json_to_md_wiki(ev.get("event_title"), sections)
        else:
            content_md = self._json_to_md_classic(ev.get("event_title"), sections)

        # è¿”å›ç»“æ„ï¼šåŒ…å« section JSON å’Œ content_md å­—ç¬¦ä¸²
        return {
            "sections": sections,
            "content_md": content_md,  # <--- å°†ç”¨äºå­˜å…¥æ•°æ®åº“
            "stats": stats_obj
        }

    def build_report_for_event(self, ev):
        # 1. æ”¶é›†æ•°æ® (ä¸ç»˜å›¾)
        news, base_fmt, stats_obj = self._collect_news_stats(ev)

        # 2. æ„å»ºäº‹å®åº“
        fact_bank_text, references, allowed_dates = build_fact_bank(news, ev.get("keywords", []))
        base_fmt = dict(base_fmt, fact_bank=fact_bank_text)

        # 3. ç”ŸæˆæŠ¥å‘Š (ä¸å†™æ–‡ä»¶)
        classic = self._gen_one_style(ev, base_fmt, stats_obj, references, allowed_dates, style="classic")
        wiki = self._gen_one_style(ev, base_fmt, stats_obj, references, allowed_dates, style="wiki")

        # 4. æ›´æ–°æ•°æ®åº“
        self.col_out.update_one(
            {"event_id": ev["event_id"]},
            {"$set": {
                "report": {"classic": classic, "wiki": wiki},
                "updated_at": dt.datetime.utcnow()
            }}
        )

    def run(self):
        docs = self.load_recent_docs()
        if not docs:
            print("No docs in window; relax filter or check fields.")
            return
        events = self.build_events(docs)
        if not events:
            print("No clusters formed.")
            return
        for ev in events:
            self.upsert_event(ev)
        print(f"[Batch] ç”Ÿæˆ/æ›´æ–°äº‹ä»¶ï¼š{len(events)}")

        for ev in events:
            try:
                self.build_report_for_event(ev)
            except Exception as e:
                print(f"[WARN] æŠ¥å‘Šå¤±è´¥ {ev['event_id']}: {e}")
        print("[Done] æŠ¥å‘Šå·²æ›´æ–°è‡³æ•°æ®åº“ã€‚")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--noop", action="store_true", help="åªæµ‹è¯•è¿æ¥")
    args = parser.parse_args()

    app = HotEventsTwoReportsStrict()
    if args.noop: return
    app.run()


# ä¼˜åŒ–åçš„å…¥å£å‡½æ•°
def run_on_collection(collection_name=None, source_collections=None):
    """
    collection_name: åºŸå¼ƒï¼Œä¿ç•™å…¼å®¹
    source_collections: æŒ‡å®šè¦æ‰«æçš„æºé›†åˆåˆ—è¡¨
    """
    target_sources = source_collections if source_collections else settings.COL_SRC_LIST
    print(f"ğŸ”¥ å¼€å§‹çƒ­ç‚¹äº‹ä»¶è¯†åˆ« (Sources: {target_sources})")
    app = HotEventsTwoReportsStrict(source_collections=target_sources)
    app.run()
    print("ğŸ”¥ çƒ­ç‚¹äº‹ä»¶è¯†åˆ«å®Œæˆ")


if __name__ == "__main__":
    main()