import json
import re
import time
import asyncio
import traceback
from pymongo import MongoClient, UpdateOne
from bson import ObjectId

# === å¯¼å…¥é…ç½® ===
from config.settings import settings
from common.utils import get_logger

# === å¯¼å…¥åŠŸèƒ½å­æ¨¡å— ===
from semantic.conversion import conversion
from semantic.Anaphora_Resolution import Disambiguation
from semantic.Text_Extraction import event
from semantic.Hot_topic import hotspot
from semantic.Into_mongodb import mogongdb
from semantic.Time_Standard import event_time
from semantic.Abstract import abstract
from semantic.Images import images
# å¯¼å…¥ Nebula å¯¼å…¥æ¨¡å—
from semantic.Into_nebula import nebula_import

logger = get_logger(__name__)


class SemanticPipeline:
    def __init__(self):
        self.client = MongoClient(settings.MONGO_URI)
        self.db = self.client[settings.MONGO_DB_NAME]

        # åˆå§‹åŒ–é›†åˆå¯¹è±¡
        self.interim_col = self.db[settings.COLL_INTERIM]  # interim (å­˜æ”¾æœ¬æ‰¹æ¬¡å¾…å¤„ç†å¢é‡æ•°æ®)
        self.detail_col = self.db[settings.INTERIM_COLLECTION]  # toutiao_news_event
        self.new_detail_col = self.db[settings.COL_EVOLUTION]  # evolution_event
        self.event_node_col = self.db[settings.EVENT_NODE_COLLECTION]  # extract_element_event

        # æºæ•°æ®é›†åˆå­—å…¸
        self.col_src_dict = {name: self.db[name] for name in settings.COL_SRC_LIST}

        self.running = False  # æ§åˆ¶å¾ªç¯æ ‡å¿—
        self.last_run_time = time.time()  # ä¸Šæ¬¡è¿è¡Œæ—¶é—´

        # === è§¦å‘é˜ˆå€¼é…ç½® (ä¼˜å…ˆä» settings è¯»å–ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼) ===
        self.TRIGGER_COUNT = getattr(settings, 'TRIGGER_DOC_COUNT', 50)  # ç§¯æ”’å¤šå°‘æ¡è§¦å‘
        self.TRIGGER_WAIT = getattr(settings, 'TRIGGER_MAX_WAIT_SECONDS', 1800)  # æœ€é•¿ç­‰å¾…ç§’æ•° (30åˆ†é’Ÿ)

    def get_max_event_ids(self):
        """è·å–å„ç±»å‹äº‹ä»¶çš„æœ€å¤§ç¼–å·ï¼ˆä» extract_element_event é›†åˆç»Ÿè®¡ï¼‰"""
        max_ids = {}
        all_events = self.event_node_col.aggregate([
            {"$match": {"event_id": {"$exists": True}}},
            {"$group": {"_id": None, "ids": {"$addToSet": "$event_id"}}}
        ])

        prefix_pattern = re.compile(r"^(Th_(ECON|MIL|POL))-(\d+)$")

        for doc in all_events:
            for eid in doc.get("ids", []):
                if not eid: continue
                match = prefix_pattern.match(eid)
                if not match: continue
                prefix, _, num = match.groups()
                try:
                    num = int(num)
                except ValueError:
                    continue
                if prefix not in max_ids or num > max_ids[prefix]:
                    max_ids[prefix] = num
        return max_ids

    def reassign_event_ids(self, docs, max_ids):
        """ä¸ºæ–°æ–‡æ¡£åˆ†é…æ–°çš„äº‹ä»¶ID"""
        prefix_pattern = re.compile(r"^(Th_(ECON|MIL|POL))-(\d+)$")
        id_mapping = {}

        for item in docs:
            events = item.get("structured_data", {}).get("events", [])
            relations = item.get("structured_data", {}).get("event_relations", [])

            for event_item in events:
                eid = event_item.get("event_id")
                if not eid: continue
                match = prefix_pattern.match(eid)
                if not match: continue

                prefix, _, _ = match.groups()
                # è‡ªå¢ ID
                max_ids[prefix] = max_ids.get(prefix, 0) + 1
                new_id = f"{prefix}-{max_ids[prefix]:05d}"

                id_mapping[eid] = new_id
                event_item["event_id"] = new_id

            # æ›´æ–°å…³ç³»ä¸­çš„ ID
            new_relations = []
            for r in relations:
                parts = r.strip().split()
                if len(parts) != 3: continue
                src, tgt, rtype = parts
                src = id_mapping.get(src, src)
                tgt = id_mapping.get(tgt, tgt)
                if src != tgt:
                    new_relations.append(f"{src}  {tgt}  {rtype}")
            item["structured_data"]["event_relations"] = new_relations

        return docs, id_mapping

    def has_valid_time(self, event_item):
        """æ£€æŸ¥äº‹ä»¶æ˜¯å¦åŒ…å«æœ‰æ•ˆæ—¶é—´"""
        if not isinstance(event_item, dict): return False
        time_pos = event_item.get("time_position") or event_item.get("time_position_moment") or event_item.get(
            "time_position_period")
        if not time_pos: return False
        if isinstance(time_pos, str) and time_pos.strip(): return True
        if isinstance(time_pos, dict) and any(v for v in time_pos.values()): return True
        return False

    def clean_structured_data(self, structured_data):
        """é€’å½’æ¸…ç†ç©ºå­—æ®µ"""

        def _clean(data):
            if isinstance(data, dict):
                return {k: _clean(v) for k, v in data.items()
                        if v not in ["", None, [], {}] and k not in ["trigger_word", "role", "emotion"]}
            elif isinstance(data, list):
                return [_clean(v) for v in data if v not in ["", None, [], {}]]
            else:
                return data

        if not isinstance(structured_data, dict): return {}
        return _clean(structured_data)

    def to_object_id(self, id_val):
        if isinstance(id_val, ObjectId): return id_val
        try:
            return ObjectId(str(id_val))
        except:
            return ObjectId()

    # --- æ ¸å¿ƒé€»è¾‘ä¼˜åŒ–ï¼šè§¦å‘æ£€æŸ¥ ---
    def check_trigger_condition(self):
        """
        æ£€æŸ¥æ˜¯å¦æ»¡è¶³è§¦å‘æ¡ä»¶ï¼š
        1. ç§¯å‹æ€»æ•° >= TRIGGER_COUNT
        2. ç­‰å¾…æ—¶é—´ >= TRIGGER_WAIT ä¸”æœ‰æ•°æ®
        """
        total_new = 0
        for col_name in settings.COL_SRC_LIST:
            # ç»Ÿè®¡ status='0' (æœªå¤„ç†)
            cnt = self.col_src_dict[col_name].count_documents({"status": "0"})
            total_new += cnt

        elapsed = time.time() - self.last_run_time
        should_run = False
        reason = ""

        if total_new >= self.TRIGGER_COUNT:
            should_run = True
            reason = f"æ•°é‡é˜ˆå€¼è§¦å‘ (ç§¯å‹ {total_new} æ¡)"
        elif total_new > 0 and elapsed >= self.TRIGGER_WAIT:
            should_run = True
            reason = f"æ—¶é—´é˜ˆå€¼è§¦å‘ (ç­‰å¾… {int(elapsed)}s, ç§¯å‹ {total_new} æ¡)"

        return should_run, reason, total_new

    # --- æ ¸å¿ƒæ‰§è¡Œé€»è¾‘ ---
    def run_once(self, force=False):
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„ Pipeline (æ”¯æŒç§¯æ”’è§¦å‘ + å¢é‡å¤„ç†)"""

        # 1. æ£€æŸ¥è§¦å‘æ¡ä»¶
        if not force:
            should_run, reason, total_new = self.check_trigger_condition()
            if not should_run:
                # logger.debug(f"[Semantic] æœªæ»¡è¶³è§¦å‘æ¡ä»¶ (ç§¯å‹: {total_new})")
                return f"Skipped: Not enough data ({total_new})"
            logger.info(f"ğŸš€ [Semantic] è§¦å‘æ‰§è¡Œ: {reason}")
        else:
            logger.info("ğŸš€ [Semantic] å¼ºåˆ¶è§¦å‘æ‰§è¡Œ...")

        self.last_run_time = time.time()

        # 2. æ•°æ®æ¬è¿ï¼šæºé›†åˆ(status=0) -> Interimï¼Œå¹¶æ ‡è®°æº status=1
        self.interim_col.delete_many({})  # æ¸…ç©º interimï¼Œå‡†å¤‡æ¥æ”¶æœ¬æ‰¹æ¬¡å¢é‡

        moved_ids_map = {}  # {col_name: [ids...]}
        total_moved = 0

        for col_name in settings.COL_SRC_LIST:
            # è·å–è¯¥é›†åˆçš„ä¸€æ‰¹æ–°æ•°æ®
            # é™åˆ¶ä¸€æ¬¡å¤„ç†é‡ï¼Œé˜²æ­¢å•æ¬¡è¿‡å¤š
            batch_limit = getattr(settings, 'SEMANTIC_BATCH_SIZE', 200)
            docs = list(self.col_src_dict[col_name].find({"status": "0"}).limit(batch_limit))

            if docs:
                self.interim_col.insert_many(docs)
                ids = [d["_id"] for d in docs]
                moved_ids_map[col_name] = ids
                total_moved += len(docs)

        if total_moved == 0 and not force:
            return "No new data moved"

        # 3. æ ‡è®°æºæ•°æ®ä¸º "1" (å¤„ç†ä¸­/å·²å¤„ç†)ï¼Œé˜²æ­¢é‡å¤æ¬è¿
        for col_name, ids in moved_ids_map.items():
            if ids:
                self.col_src_dict[col_name].update_many(
                    {"_id": {"$in": ids}},
                    {"$set": {"status": "1"}}
                )

        logger.info(f"ğŸ“¥ [Semantic] æœ¬æ¬¡å¢é‡å¤„ç†æ•°æ®: {total_moved} æ¡")

        try:
            # 4. è°ƒç”¨å„ä¸ªå­æ¨¡å—

            # [Step 1] çƒ­ç‚¹äº‹ä»¶è¯†åˆ«
            # ä¼˜åŒ–ï¼šä¸åªçœ‹ interimï¼Œè€Œæ˜¯æ‰«ææ‰€æœ‰æºé›†åˆçš„æœ€è¿‘ N å¤©æ•°æ®ï¼Œä¿è¯çƒ­ç‚¹è¿è´¯æ€§
            logger.info("ğŸ”¥ [Step 1] çƒ­ç‚¹äº‹ä»¶è¯†åˆ« (æ‰«æå…¨é‡æºä¸Šä¸‹æ–‡)...")
            # ä¼ å…¥æºé›†åˆåˆ—è¡¨ï¼Œhotspot æ¨¡å—ä¼šå»éå†è¿™äº›é›†åˆ
            hotspot.run_on_collection(source_collections=settings.COL_SRC_LIST)

            # [Step 2] å¢é‡ä¿¡æ¯æŠ½å– (ä»…é’ˆå¯¹ interim ä¸­çš„æ–°æ•°æ®)
            logger.info("ğŸ§  [Step 2] å¢é‡ä¿¡æ¯æŠ½å–...")
            event.run_on_collection(settings.COLL_INTERIM)

            # [Step 3] æŒ‡ä»£æ¶ˆè§£ (ä»…é’ˆå¯¹ interim)
            logger.info("ğŸ”— [Step 3] æŒ‡ä»£æ¶ˆè§£...")
            Disambiguation.main(collection_name=settings.COLL_INTERIM)

            # [Step 4] IDåˆ†é…ä¸åˆ†å‘ (å°†æŠ½å–ç»“æœå…¥åº“)
            logger.info("ğŸ†” [Step 4] ID åˆ†é…ä¸æ ¼å¼è½¬æ¢...")
            processed_docs = list(self.interim_col.find({}))
            max_ids = self.get_max_event_ids()
            processed_docs, _ = self.reassign_event_ids(processed_docs, max_ids)

            detail_docs = []
            evo_docs = []

            for item in processed_docs:
                s_data = self.clean_structured_data(item.get("structured_data", {}))
                # è¿‡æ»¤æ— æ—¶é—´äº‹ä»¶
                valid_events = [e for e in s_data.get("events", []) if self.has_valid_time(e)]
                if not valid_events: continue
                s_data["events"] = valid_events

                # æ„é€ åŸå§‹ç»“æ„æ•°æ®
                detail_docs.append({
                    "_id": self.to_object_id(item.get("_id")),
                    "source": item.get("source"),
                    "event_first_level": item.get("predicted_category"),
                    "event_second_level": item.get("predicted_subcategory"),
                    "structured_data": s_data
                })
                # æ„é€ æ¼”åŒ–ç»“æ„æ•°æ®
                evo_docs.append(conversion.convert_document_simple(item))

            # å†™å…¥ç»“æœè¡¨
            if detail_docs:
                for d in detail_docs:
                    self.detail_col.replace_one({"_id": d["_id"]}, d, upsert=True)
            if evo_docs:
                for d in evo_docs:
                    self.new_detail_col.replace_one({"_id": d["_id"]}, d, upsert=True)

            logger.info(f"ğŸ’¾ [Step 5] æ•°æ®å…¥åº“å®Œæˆ (Detail: {len(detail_docs)}, Evo: {len(evo_docs)})")

            # [Step 6] å›¾è°±æ„å»º (ä»åº“ä¸­è¯»å–æ•°æ®æ„å»ºå…³è”)
            logger.info("ğŸ•¸ï¸ [Step 6] å›¾è°±æ„å»º (Mogongdb)...")
            mogongdb.main()

            # [Step 7] æ—¶é—´æ ‡å‡†åŒ–
            logger.info("â±ï¸ [Step 7] æ—¶é—´æ ‡å‡†åŒ–...")
            # è¿ç§»æ—§å­—æ®µå…¼å®¹
            self.db[settings.EVENT_NODE_COLLECTION].update_many(
                {"time_position_period": {"$exists": True}},
                [{"$set": {"time_position_moment": "$time_position_period", "time_position_period": "$$REMOVE"}}]
            )
            event_time.run_update(limit=0)

            # [Step 8] ç”Ÿæˆæ‘˜è¦
            logger.info("ğŸ“ [Step 8] ç”Ÿæˆæ‘˜è¦...")
            abstract.main()

            # [Step 9] å›¾ç‰‡å¤„ç† (æŒ‰éœ€å¼€å¯)
            logger.info("ğŸ–¼ï¸ [Step 9] å›¾ç‰‡å¤„ç†...")
            # images.main()

            # [Step 10] Nebula å…¥åº“
            logger.info("ğŸŒŒ [Step 10] Nebula å›¾è°±å¯¼å…¥...")
            nebula_import.main()

            # [Step 11] æ¸…ç†ä¸´æ—¶é›†åˆ
            logger.info("ğŸ§¹ [Step 11] æ¸…ç©ºä¸´æ—¶é›†åˆ interim / toutiao_news_event ...")
            self.interim_col.drop()
            self.detail_col.drop()

            logger.info("âœ… [Semantic] æµç¨‹ç»“æŸ")
            return "Success"

        except Exception as e:
            logger.error(f"âŒ [Semantic] å¤„ç†æµç¨‹å¼‚å¸¸: {e}\n{traceback.format_exc()}")
            # å‡ºé”™ä¿ç•™ status="1" ä»¥ä¾¿äººå·¥æ’æŸ¥ï¼Œæˆ–è€…å¯é€‰æ‹©å›æ»šä¸º "0"
            return f"Error: {str(e)}"

    async def run_loop(self, interval=10):
        """åå°è‡ªåŠ¨å¾ªç¯ä»»åŠ¡"""
        logger.info(f"ğŸ”„ [Semantic] è‡ªåŠ¨ç›‘æ§å·²å¯åŠ¨ (æ£€æµ‹é¢‘ç‡: {interval}s)")
        self.running = True
        while self.running:
            try:
                # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥ä»»åŠ¡ï¼Œé¿å…é˜»å¡
                await asyncio.to_thread(self.run_once, force=False)
            except Exception as e:
                logger.error(f"Loop Error: {e}")

            # ç­‰å¾…é—´éš” (ä½¿ç”¨è¾ƒçŸ­é—´éš”ä»¥ä¾¿åŠæ—¶å“åº”å¼ºåˆ¶è§¦å‘æˆ–è¾¾åˆ°é˜ˆå€¼)
            for _ in range(interval):
                if not self.running: break
                await asyncio.sleep(1)

        logger.info("ğŸ›‘ [Semantic] è‡ªåŠ¨ç›‘æ§å·²åœæ­¢")

    def stop(self):
        self.running = False


# å…¨å±€å•ä¾‹
pipeline_instance = SemanticPipeline()