nohup: ignoring input
2025-12-11 15:16:33 | INFO | tools.text_annotation.api | æ­£åœ¨åˆå§‹åŒ–æé€Ÿæ–‡æœ¬æ ‡æ³¨æ¨¡å‹: qwen3:0.6b ...
2025-12-11 15:16:33 | INFO | tools.text_annotation.api | âœ… æ–‡æœ¬æ ‡æ³¨æ¨¡å‹ (qwen3:0.6b) åˆå§‹åŒ–å®Œæˆ (å‡è¡¡æ¨¡å¼)
/home/vipuser/miniconda3/envs/kaiyuan/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
WARNING:  Current configuration will not reload as not all conditions are met, please refer to documentation.
INFO:     Started server process [2495741]
INFO:     Waiting for application startup.
>>> [KG Module] æœåŠ¡åˆå§‹åŒ–...
2025-12-11 15:16:39 | INFO | nebula3.logger | Get connection to ('39.104.200.88', 41003)
`torch_dtype` is deprecated! Use `dtype` instead!
    NebulaGraph è¿æ¥æˆåŠŸ (ç”¨äºåç§°è§£æå…œåº•)
    æ­£åœ¨æ„å»º åç§°-ID ç´¢å¼•...
    ç´¢å¼•æ„å»ºå®Œæˆ: 4964 ä¸ªåç§°æŒ‡å‘ 2808 ä¸ªå®ä½“ã€‚
    [Sample Index] [('Th_POL-04157', 'Th_POL-04157'), ('ä¸­æ–¹è¦æ±‚æ—¥æ–¹æ”¶å›æ¶‰å°é”™è¯¯è¨€è®º', 'Th_POL-04057'), ('Th_POL-04032', 'Th_POL-04032'), ('æ—¥æœ¬é¦–ç›¸é«˜å¸‚æ—©è‹—å‘è¡¨æ¶‰å°æŒ‘è¡…è¨€è®º', 'Th_POL-02021'), ('Th_POL-01864', 'Th_POL-01864')]
    Loading TransE vectors...
    Loading LLM (Base + LoRA)...
2025-12-11 15:16:41 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:06,  1.58s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  1.89s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:05<00:03,  1.71s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:07<00:01,  1.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:08<00:00,  1.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:08<00:00,  1.62s/it]
    LLM åŠ è½½æˆåŠŸ
>>> [KG Module] åˆå§‹åŒ–å®Œæˆ
>>> [KGE] æ­£åœ¨åŠ è½½ Tokenizer...
>>> [KGE] æ­£åœ¨åŠ è½½ Model...
2025-12-11 15:16:50 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.32it/s]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.38it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.40it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:02<00:00,  1.53it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.04it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.71it/s]
/home/vipuser/miniconda3/envs/kaiyuan/lib/python3.11/site-packages/peft/utils/save_and_load.py:546: UserWarning: Some weights of PeftModelForCausalLM were not initialized from the model checkpoint and are being ignored because you passed `ignore_mismatched_sizes=True`: - base_model.model.lm_head.weight: found shape torch.Size([151670, 4096]) in the checkpoint and torch.Size([151936, 4096]) in the model instantiated
- base_model.model.model.embed_tokens.weight: found shape torch.Size([151670, 4096]) in the checkpoint and torch.Size([151936, 4096]) in the model instantiated.
  warnings.warn(msg)
âœ… [KGE] LoRA Adapter åŠ è½½æˆåŠŸ
2025-12-11 15:16:55 | INFO | tools.api_tools | æ­£åœ¨åˆå§‹åŒ–å·¥å…·é›†é¢„çƒ­ä»»åŠ¡...
INFO:     Application startup complete.
2025-12-11 15:16:55 | INFO | tools.text_annotation.api | ğŸ”¥ [TextLabel-Speed] é¢„çƒ­ä¸­...
2025-12-11 15:16:55 | INFO | tools.word_cloud.api | ğŸ”¥ [WordCloud] æ­£åœ¨åŠ è½½ Jieba å­—å…¸...
Building prefix dict from the default dictionary ...
2025-12-11 15:16:55 | DEBUG | jieba | Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
2025-12-11 15:16:55 | DEBUG | jieba | Loading model from cache /tmp/jieba.cache
Loading model cost 0.728 seconds.
2025-12-11 15:16:56 | DEBUG | jieba | Loading model cost 0.728 seconds.
Prefix dict has been built successfully.
2025-12-11 15:16:56 | DEBUG | jieba | Prefix dict has been built successfully.
2025-12-11 15:16:56 | INFO | tools.word_cloud.api | âœ… [WordCloud] Jieba å­—å…¸åŠ è½½å®Œæˆ
2025-12-11 15:16:56 | INFO | tools.image_description.api | ğŸ”¥ [ImageDesc] å¼€å§‹åŠ è½½å¤šæ¨¡æ€æ¨¡å‹...
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Device set to use cuda:0
2025-12-11 15:16:59 | INFO | tools.image_description.api | âœ… LLM (qwen3:32b) è¿æ¥æˆåŠŸ
2025-12-11 15:16:59 | INFO | tools.image_description.api | âœ… [ImageDesc] æ¨¡å‹åŠ è½½æµç¨‹ç»“æŸ
INFO:     Uvicorn running on http://0.0.0.0:8801 (Press CTRL+C to quit)
2025-12-11 15:17:02 | INFO | tools.text_annotation.api | âœ… [TextLabel-Speed] é¢„çƒ­å®Œæˆ
INFO:     117.71.104.18:29779 - "GET /tools/wordcloud/generate HTTP/1.1" 200 OK
INFO:     117.71.104.18:61088 - "GET /tools/wordcloud/generate?keyword=ggg HTTP/1.1" 200 OK
INFO:     117.71.104.18:26587 - "POST /tools/capture/run HTTP/1.1" 200 OK
